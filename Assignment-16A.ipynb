{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f966ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ffdd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>aug</td>\n",
       "      <td>sat</td>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>nov</td>\n",
       "      <td>tue</td>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    month  day  FFMC    DMC     DC   ISI  temp  RH  wind  rain  ...  monthfeb  \\\n",
       "0     mar  fri  86.2   26.2   94.3   5.1   8.2  51   6.7   0.0  ...         0   \n",
       "1     oct  tue  90.6   35.4  669.1   6.7  18.0  33   0.9   0.0  ...         0   \n",
       "2     oct  sat  90.6   43.7  686.9   6.7  14.6  33   1.3   0.0  ...         0   \n",
       "3     mar  fri  91.7   33.3   77.5   9.0   8.3  97   4.0   0.2  ...         0   \n",
       "4     mar  sun  89.3   51.3  102.2   9.6  11.4  99   1.8   0.0  ...         0   \n",
       "..    ...  ...   ...    ...    ...   ...   ...  ..   ...   ...  ...       ...   \n",
       "512   aug  sun  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0  ...         0   \n",
       "513   aug  sun  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  ...         0   \n",
       "514   aug  sun  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  ...         0   \n",
       "515   aug  sat  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0  ...         0   \n",
       "516   nov  tue  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0  ...         0   \n",
       "\n",
       "     monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  monthoct  \\\n",
       "0           0         0         0         1         0         0         0   \n",
       "1           0         0         0         0         0         0         1   \n",
       "2           0         0         0         0         0         0         1   \n",
       "3           0         0         0         1         0         0         0   \n",
       "4           0         0         0         1         0         0         0   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512         0         0         0         0         0         0         0   \n",
       "513         0         0         0         0         0         0         0   \n",
       "514         0         0         0         0         0         0         0   \n",
       "515         0         0         0         0         0         0         0   \n",
       "516         0         0         0         0         0         1         0   \n",
       "\n",
       "     monthsep  size_category  \n",
       "0           0          small  \n",
       "1           0          small  \n",
       "2           0          small  \n",
       "3           0          small  \n",
       "4           0          small  \n",
       "..        ...            ...  \n",
       "512         0          large  \n",
       "513         0          large  \n",
       "514         0          large  \n",
       "515         0          small  \n",
       "516         0          small  \n",
       "\n",
       "[517 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"forestfires.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b30090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 31 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   month          517 non-null    object \n",
      " 1   day            517 non-null    object \n",
      " 2   FFMC           517 non-null    float64\n",
      " 3   DMC            517 non-null    float64\n",
      " 4   DC             517 non-null    float64\n",
      " 5   ISI            517 non-null    float64\n",
      " 6   temp           517 non-null    float64\n",
      " 7   RH             517 non-null    int64  \n",
      " 8   wind           517 non-null    float64\n",
      " 9   rain           517 non-null    float64\n",
      " 10  area           517 non-null    float64\n",
      " 11  dayfri         517 non-null    int64  \n",
      " 12  daymon         517 non-null    int64  \n",
      " 13  daysat         517 non-null    int64  \n",
      " 14  daysun         517 non-null    int64  \n",
      " 15  daythu         517 non-null    int64  \n",
      " 16  daytue         517 non-null    int64  \n",
      " 17  daywed         517 non-null    int64  \n",
      " 18  monthapr       517 non-null    int64  \n",
      " 19  monthaug       517 non-null    int64  \n",
      " 20  monthdec       517 non-null    int64  \n",
      " 21  monthfeb       517 non-null    int64  \n",
      " 22  monthjan       517 non-null    int64  \n",
      " 23  monthjul       517 non-null    int64  \n",
      " 24  monthjun       517 non-null    int64  \n",
      " 25  monthmar       517 non-null    int64  \n",
      " 26  monthmay       517 non-null    int64  \n",
      " 27  monthnov       517 non-null    int64  \n",
      " 28  monthoct       517 non-null    int64  \n",
      " 29  monthsep       517 non-null    int64  \n",
      " 30  size_category  517 non-null    object \n",
      "dtypes: float64(8), int64(20), object(3)\n",
      "memory usage: 125.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86122603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FFMC</th>\n",
       "      <td>517.0</td>\n",
       "      <td>90.644681</td>\n",
       "      <td>5.520111</td>\n",
       "      <td>18.7</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.60</td>\n",
       "      <td>92.90</td>\n",
       "      <td>96.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMC</th>\n",
       "      <td>517.0</td>\n",
       "      <td>110.872340</td>\n",
       "      <td>64.046482</td>\n",
       "      <td>1.1</td>\n",
       "      <td>68.6</td>\n",
       "      <td>108.30</td>\n",
       "      <td>142.40</td>\n",
       "      <td>291.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DC</th>\n",
       "      <td>517.0</td>\n",
       "      <td>547.940039</td>\n",
       "      <td>248.066192</td>\n",
       "      <td>7.9</td>\n",
       "      <td>437.7</td>\n",
       "      <td>664.20</td>\n",
       "      <td>713.90</td>\n",
       "      <td>860.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISI</th>\n",
       "      <td>517.0</td>\n",
       "      <td>9.021663</td>\n",
       "      <td>4.559477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.40</td>\n",
       "      <td>10.80</td>\n",
       "      <td>56.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>517.0</td>\n",
       "      <td>18.889168</td>\n",
       "      <td>5.806625</td>\n",
       "      <td>2.2</td>\n",
       "      <td>15.5</td>\n",
       "      <td>19.30</td>\n",
       "      <td>22.80</td>\n",
       "      <td>33.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RH</th>\n",
       "      <td>517.0</td>\n",
       "      <td>44.288201</td>\n",
       "      <td>16.317469</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>53.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind</th>\n",
       "      <td>517.0</td>\n",
       "      <td>4.017602</td>\n",
       "      <td>1.791653</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>9.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rain</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>0.295959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>517.0</td>\n",
       "      <td>12.847292</td>\n",
       "      <td>63.655818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>6.57</td>\n",
       "      <td>1090.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dayfri</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.164410</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daymon</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.143133</td>\n",
       "      <td>0.350548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysat</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.162476</td>\n",
       "      <td>0.369244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysun</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.183752</td>\n",
       "      <td>0.387657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daythu</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.117988</td>\n",
       "      <td>0.322907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daytue</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.123791</td>\n",
       "      <td>0.329662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daywed</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.104449</td>\n",
       "      <td>0.306138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthapr</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.130913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthaug</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.355899</td>\n",
       "      <td>0.479249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthdec</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.130913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthfeb</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>0.193029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjan</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjul</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.061896</td>\n",
       "      <td>0.241199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjun</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthmar</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.104449</td>\n",
       "      <td>0.306138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthmay</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthnov</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.043980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthoct</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.168007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthsep</th>\n",
       "      <td>517.0</td>\n",
       "      <td>0.332689</td>\n",
       "      <td>0.471632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count        mean         std   min    25%     50%     75%      max\n",
       "FFMC      517.0   90.644681    5.520111  18.7   90.2   91.60   92.90    96.20\n",
       "DMC       517.0  110.872340   64.046482   1.1   68.6  108.30  142.40   291.30\n",
       "DC        517.0  547.940039  248.066192   7.9  437.7  664.20  713.90   860.60\n",
       "ISI       517.0    9.021663    4.559477   0.0    6.5    8.40   10.80    56.10\n",
       "temp      517.0   18.889168    5.806625   2.2   15.5   19.30   22.80    33.30\n",
       "RH        517.0   44.288201   16.317469  15.0   33.0   42.00   53.00   100.00\n",
       "wind      517.0    4.017602    1.791653   0.4    2.7    4.00    4.90     9.40\n",
       "rain      517.0    0.021663    0.295959   0.0    0.0    0.00    0.00     6.40\n",
       "area      517.0   12.847292   63.655818   0.0    0.0    0.52    6.57  1090.84\n",
       "dayfri    517.0    0.164410    0.371006   0.0    0.0    0.00    0.00     1.00\n",
       "daymon    517.0    0.143133    0.350548   0.0    0.0    0.00    0.00     1.00\n",
       "daysat    517.0    0.162476    0.369244   0.0    0.0    0.00    0.00     1.00\n",
       "daysun    517.0    0.183752    0.387657   0.0    0.0    0.00    0.00     1.00\n",
       "daythu    517.0    0.117988    0.322907   0.0    0.0    0.00    0.00     1.00\n",
       "daytue    517.0    0.123791    0.329662   0.0    0.0    0.00    0.00     1.00\n",
       "daywed    517.0    0.104449    0.306138   0.0    0.0    0.00    0.00     1.00\n",
       "monthapr  517.0    0.017408    0.130913   0.0    0.0    0.00    0.00     1.00\n",
       "monthaug  517.0    0.355899    0.479249   0.0    0.0    0.00    1.00     1.00\n",
       "monthdec  517.0    0.017408    0.130913   0.0    0.0    0.00    0.00     1.00\n",
       "monthfeb  517.0    0.038685    0.193029   0.0    0.0    0.00    0.00     1.00\n",
       "monthjan  517.0    0.003868    0.062137   0.0    0.0    0.00    0.00     1.00\n",
       "monthjul  517.0    0.061896    0.241199   0.0    0.0    0.00    0.00     1.00\n",
       "monthjun  517.0    0.032882    0.178500   0.0    0.0    0.00    0.00     1.00\n",
       "monthmar  517.0    0.104449    0.306138   0.0    0.0    0.00    0.00     1.00\n",
       "monthmay  517.0    0.003868    0.062137   0.0    0.0    0.00    0.00     1.00\n",
       "monthnov  517.0    0.001934    0.043980   0.0    0.0    0.00    0.00     1.00\n",
       "monthoct  517.0    0.029014    0.168007   0.0    0.0    0.00    0.00     1.00\n",
       "monthsep  517.0    0.332689    0.471632   0.0    0.0    0.00    1.00     1.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7a6793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthdec</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FFMC</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.330512</td>\n",
       "      <td>0.531805</td>\n",
       "      <td>0.431532</td>\n",
       "      <td>-0.300995</td>\n",
       "      <td>-0.028485</td>\n",
       "      <td>0.056702</td>\n",
       "      <td>0.040122</td>\n",
       "      <td>0.019306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137044</td>\n",
       "      <td>-0.281535</td>\n",
       "      <td>-0.454771</td>\n",
       "      <td>0.031833</td>\n",
       "      <td>-0.040634</td>\n",
       "      <td>-0.074327</td>\n",
       "      <td>-0.037230</td>\n",
       "      <td>-0.088964</td>\n",
       "      <td>-0.005998</td>\n",
       "      <td>0.076609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMC</th>\n",
       "      <td>0.382619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.682192</td>\n",
       "      <td>0.305128</td>\n",
       "      <td>0.469594</td>\n",
       "      <td>0.073795</td>\n",
       "      <td>-0.105342</td>\n",
       "      <td>0.074790</td>\n",
       "      <td>0.072994</td>\n",
       "      <td>-0.012010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176301</td>\n",
       "      <td>-0.317899</td>\n",
       "      <td>-0.105647</td>\n",
       "      <td>-0.001946</td>\n",
       "      <td>-0.050403</td>\n",
       "      <td>-0.407404</td>\n",
       "      <td>-0.081980</td>\n",
       "      <td>-0.074218</td>\n",
       "      <td>-0.187632</td>\n",
       "      <td>0.110907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DC</th>\n",
       "      <td>0.330512</td>\n",
       "      <td>0.682192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.229154</td>\n",
       "      <td>0.496208</td>\n",
       "      <td>-0.039192</td>\n",
       "      <td>-0.203466</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105642</td>\n",
       "      <td>-0.399277</td>\n",
       "      <td>-0.115064</td>\n",
       "      <td>-0.100887</td>\n",
       "      <td>-0.186183</td>\n",
       "      <td>-0.650427</td>\n",
       "      <td>-0.114209</td>\n",
       "      <td>-0.078380</td>\n",
       "      <td>0.093279</td>\n",
       "      <td>0.531857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISI</th>\n",
       "      <td>0.531805</td>\n",
       "      <td>0.305128</td>\n",
       "      <td>0.229154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.394287</td>\n",
       "      <td>-0.132517</td>\n",
       "      <td>0.106826</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.046695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162322</td>\n",
       "      <td>-0.249777</td>\n",
       "      <td>-0.103588</td>\n",
       "      <td>0.020982</td>\n",
       "      <td>0.111516</td>\n",
       "      <td>-0.143520</td>\n",
       "      <td>-0.060493</td>\n",
       "      <td>-0.076559</td>\n",
       "      <td>-0.071154</td>\n",
       "      <td>-0.068877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>0.431532</td>\n",
       "      <td>0.469594</td>\n",
       "      <td>0.496208</td>\n",
       "      <td>0.394287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.527390</td>\n",
       "      <td>-0.227116</td>\n",
       "      <td>0.069491</td>\n",
       "      <td>0.097844</td>\n",
       "      <td>-0.071949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329648</td>\n",
       "      <td>-0.320015</td>\n",
       "      <td>-0.146520</td>\n",
       "      <td>0.142588</td>\n",
       "      <td>0.051015</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>-0.045540</td>\n",
       "      <td>-0.053798</td>\n",
       "      <td>-0.053513</td>\n",
       "      <td>0.088006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RH</th>\n",
       "      <td>-0.300995</td>\n",
       "      <td>0.073795</td>\n",
       "      <td>-0.039192</td>\n",
       "      <td>-0.132517</td>\n",
       "      <td>-0.527390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069410</td>\n",
       "      <td>0.099751</td>\n",
       "      <td>-0.075519</td>\n",
       "      <td>0.064506</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>0.140430</td>\n",
       "      <td>0.170923</td>\n",
       "      <td>0.013185</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>-0.089836</td>\n",
       "      <td>0.086822</td>\n",
       "      <td>-0.035885</td>\n",
       "      <td>-0.072334</td>\n",
       "      <td>-0.062596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind</th>\n",
       "      <td>-0.028485</td>\n",
       "      <td>-0.105342</td>\n",
       "      <td>-0.203466</td>\n",
       "      <td>0.106826</td>\n",
       "      <td>-0.227116</td>\n",
       "      <td>0.069410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061119</td>\n",
       "      <td>0.012317</td>\n",
       "      <td>0.118090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269702</td>\n",
       "      <td>-0.029431</td>\n",
       "      <td>-0.070245</td>\n",
       "      <td>-0.040645</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>0.181433</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>-0.053850</td>\n",
       "      <td>-0.181476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rain</th>\n",
       "      <td>0.056702</td>\n",
       "      <td>0.074790</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.069491</td>\n",
       "      <td>0.099751</td>\n",
       "      <td>0.061119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007366</td>\n",
       "      <td>-0.004261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.014698</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>-0.013510</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>-0.003225</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>-0.051733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>0.040122</td>\n",
       "      <td>0.072994</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.097844</td>\n",
       "      <td>-0.075519</td>\n",
       "      <td>0.012317</td>\n",
       "      <td>-0.007366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.052911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>-0.020732</td>\n",
       "      <td>-0.012589</td>\n",
       "      <td>0.006149</td>\n",
       "      <td>-0.020314</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>-0.016878</td>\n",
       "      <td>0.056573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dayfri</th>\n",
       "      <td>0.019306</td>\n",
       "      <td>-0.012010</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.046695</td>\n",
       "      <td>-0.071949</td>\n",
       "      <td>0.064506</td>\n",
       "      <td>0.118090</td>\n",
       "      <td>-0.004261</td>\n",
       "      <td>-0.052911</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019140</td>\n",
       "      <td>0.046323</td>\n",
       "      <td>-0.027643</td>\n",
       "      <td>-0.048969</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.036205</td>\n",
       "      <td>0.056423</td>\n",
       "      <td>-0.019527</td>\n",
       "      <td>-0.045585</td>\n",
       "      <td>0.107671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daymon</th>\n",
       "      <td>-0.059396</td>\n",
       "      <td>-0.107921</td>\n",
       "      <td>-0.052993</td>\n",
       "      <td>-0.158601</td>\n",
       "      <td>-0.136529</td>\n",
       "      <td>0.009376</td>\n",
       "      <td>-0.063881</td>\n",
       "      <td>-0.029945</td>\n",
       "      <td>-0.021206</td>\n",
       "      <td>-0.181293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114519</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>-0.025470</td>\n",
       "      <td>-0.013300</td>\n",
       "      <td>0.017553</td>\n",
       "      <td>0.077125</td>\n",
       "      <td>-0.025470</td>\n",
       "      <td>-0.017992</td>\n",
       "      <td>0.060975</td>\n",
       "      <td>0.039632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysat</th>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.003653</td>\n",
       "      <td>-0.035189</td>\n",
       "      <td>-0.038585</td>\n",
       "      <td>0.034899</td>\n",
       "      <td>-0.023869</td>\n",
       "      <td>-0.063799</td>\n",
       "      <td>-0.032271</td>\n",
       "      <td>0.087868</td>\n",
       "      <td>-0.195372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058625</td>\n",
       "      <td>0.020406</td>\n",
       "      <td>0.057019</td>\n",
       "      <td>0.060945</td>\n",
       "      <td>-0.022408</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>0.057019</td>\n",
       "      <td>-0.019390</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>-0.032783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysun</th>\n",
       "      <td>-0.089517</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>-0.003243</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.136220</td>\n",
       "      <td>0.027981</td>\n",
       "      <td>-0.017872</td>\n",
       "      <td>-0.020463</td>\n",
       "      <td>-0.210462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024966</td>\n",
       "      <td>0.008416</td>\n",
       "      <td>0.050887</td>\n",
       "      <td>-0.018241</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>-0.047726</td>\n",
       "      <td>-0.029568</td>\n",
       "      <td>-0.020887</td>\n",
       "      <td>0.007252</td>\n",
       "      <td>-0.048817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daythu</th>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.087672</td>\n",
       "      <td>0.051859</td>\n",
       "      <td>-0.022406</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>-0.123061</td>\n",
       "      <td>-0.062553</td>\n",
       "      <td>-0.026798</td>\n",
       "      <td>0.020121</td>\n",
       "      <td>-0.162237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002838</td>\n",
       "      <td>-0.042278</td>\n",
       "      <td>-0.022793</td>\n",
       "      <td>-0.019300</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.026885</td>\n",
       "      <td>-0.022793</td>\n",
       "      <td>-0.016101</td>\n",
       "      <td>-0.063223</td>\n",
       "      <td>0.008984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daytue</th>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>0.068610</td>\n",
       "      <td>0.035630</td>\n",
       "      <td>-0.014211</td>\n",
       "      <td>0.053396</td>\n",
       "      <td>0.139311</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.166728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005125</td>\n",
       "      <td>-0.014491</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>0.049688</td>\n",
       "      <td>-0.069308</td>\n",
       "      <td>-0.032351</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>0.117121</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>-0.028570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daywed</th>\n",
       "      <td>0.093908</td>\n",
       "      <td>0.017939</td>\n",
       "      <td>0.024803</td>\n",
       "      <td>0.125415</td>\n",
       "      <td>0.090580</td>\n",
       "      <td>-0.087508</td>\n",
       "      <td>-0.019965</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>-0.011452</td>\n",
       "      <td>-0.151487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>-0.035713</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.008985</td>\n",
       "      <td>0.043422</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.015034</td>\n",
       "      <td>0.016325</td>\n",
       "      <td>-0.053222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthapr</th>\n",
       "      <td>-0.117199</td>\n",
       "      <td>-0.197543</td>\n",
       "      <td>-0.268211</td>\n",
       "      <td>-0.106478</td>\n",
       "      <td>-0.157051</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>0.048266</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.008280</td>\n",
       "      <td>-0.019140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017717</td>\n",
       "      <td>-0.026701</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.034190</td>\n",
       "      <td>-0.024543</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>-0.023008</td>\n",
       "      <td>-0.093982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthaug</th>\n",
       "      <td>0.228103</td>\n",
       "      <td>0.497928</td>\n",
       "      <td>0.279361</td>\n",
       "      <td>0.334639</td>\n",
       "      <td>0.351404</td>\n",
       "      <td>0.054761</td>\n",
       "      <td>0.028577</td>\n",
       "      <td>0.093101</td>\n",
       "      <td>-0.004187</td>\n",
       "      <td>-0.100837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098941</td>\n",
       "      <td>-0.149116</td>\n",
       "      <td>-0.046323</td>\n",
       "      <td>-0.190937</td>\n",
       "      <td>-0.137065</td>\n",
       "      <td>-0.253859</td>\n",
       "      <td>-0.046323</td>\n",
       "      <td>-0.032724</td>\n",
       "      <td>-0.128493</td>\n",
       "      <td>-0.524858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthdec</th>\n",
       "      <td>-0.137044</td>\n",
       "      <td>-0.176301</td>\n",
       "      <td>-0.105642</td>\n",
       "      <td>-0.162322</td>\n",
       "      <td>-0.329648</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>0.269702</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>-0.019140</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.026701</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.034190</td>\n",
       "      <td>-0.024543</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>-0.023008</td>\n",
       "      <td>-0.093982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthfeb</th>\n",
       "      <td>-0.281535</td>\n",
       "      <td>-0.317899</td>\n",
       "      <td>-0.399277</td>\n",
       "      <td>-0.249777</td>\n",
       "      <td>-0.320015</td>\n",
       "      <td>0.140430</td>\n",
       "      <td>-0.029431</td>\n",
       "      <td>-0.014698</td>\n",
       "      <td>-0.020732</td>\n",
       "      <td>0.046323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>-0.051528</td>\n",
       "      <td>-0.036989</td>\n",
       "      <td>-0.068508</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>-0.034676</td>\n",
       "      <td>-0.141642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjan</th>\n",
       "      <td>-0.454771</td>\n",
       "      <td>-0.105647</td>\n",
       "      <td>-0.115064</td>\n",
       "      <td>-0.103588</td>\n",
       "      <td>-0.146520</td>\n",
       "      <td>0.170923</td>\n",
       "      <td>-0.070245</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>-0.012589</td>\n",
       "      <td>-0.027643</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.044001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjul</th>\n",
       "      <td>0.031833</td>\n",
       "      <td>-0.001946</td>\n",
       "      <td>-0.100887</td>\n",
       "      <td>0.020982</td>\n",
       "      <td>0.142588</td>\n",
       "      <td>0.013185</td>\n",
       "      <td>-0.040645</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>0.006149</td>\n",
       "      <td>-0.048969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034190</td>\n",
       "      <td>-0.051528</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.087722</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>-0.011308</td>\n",
       "      <td>-0.044402</td>\n",
       "      <td>-0.181367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjun</th>\n",
       "      <td>-0.040634</td>\n",
       "      <td>-0.050403</td>\n",
       "      <td>-0.186183</td>\n",
       "      <td>0.111516</td>\n",
       "      <td>0.051015</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>-0.013510</td>\n",
       "      <td>-0.020314</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024543</td>\n",
       "      <td>-0.036989</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062972</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>-0.031874</td>\n",
       "      <td>-0.130195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthmar</th>\n",
       "      <td>-0.074327</td>\n",
       "      <td>-0.407404</td>\n",
       "      <td>-0.650427</td>\n",
       "      <td>-0.143520</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>-0.089836</td>\n",
       "      <td>0.181433</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>0.036205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.068508</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.087722</td>\n",
       "      <td>-0.062972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.015034</td>\n",
       "      <td>-0.059034</td>\n",
       "      <td>-0.241135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthmay</th>\n",
       "      <td>-0.037230</td>\n",
       "      <td>-0.081980</td>\n",
       "      <td>-0.114209</td>\n",
       "      <td>-0.060493</td>\n",
       "      <td>-0.045540</td>\n",
       "      <td>0.086822</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.056423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.044001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthnov</th>\n",
       "      <td>-0.088964</td>\n",
       "      <td>-0.074218</td>\n",
       "      <td>-0.078380</td>\n",
       "      <td>-0.076559</td>\n",
       "      <td>-0.053798</td>\n",
       "      <td>-0.035885</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>-0.003225</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>-0.019527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>-0.011308</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>-0.015034</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007610</td>\n",
       "      <td>-0.031083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthoct</th>\n",
       "      <td>-0.005998</td>\n",
       "      <td>-0.187632</td>\n",
       "      <td>0.093279</td>\n",
       "      <td>-0.071154</td>\n",
       "      <td>-0.053513</td>\n",
       "      <td>-0.072334</td>\n",
       "      <td>-0.053850</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>-0.016878</td>\n",
       "      <td>-0.045585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023008</td>\n",
       "      <td>-0.034676</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.044402</td>\n",
       "      <td>-0.031874</td>\n",
       "      <td>-0.059034</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.007610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.122053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthsep</th>\n",
       "      <td>0.076609</td>\n",
       "      <td>0.110907</td>\n",
       "      <td>0.531857</td>\n",
       "      <td>-0.068877</td>\n",
       "      <td>0.088006</td>\n",
       "      <td>-0.062596</td>\n",
       "      <td>-0.181476</td>\n",
       "      <td>-0.051733</td>\n",
       "      <td>0.056573</td>\n",
       "      <td>0.107671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>-0.141642</td>\n",
       "      <td>-0.044001</td>\n",
       "      <td>-0.181367</td>\n",
       "      <td>-0.130195</td>\n",
       "      <td>-0.241135</td>\n",
       "      <td>-0.044001</td>\n",
       "      <td>-0.031083</td>\n",
       "      <td>-0.122053</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              FFMC       DMC        DC       ISI      temp        RH  \\\n",
       "FFMC      1.000000  0.382619  0.330512  0.531805  0.431532 -0.300995   \n",
       "DMC       0.382619  1.000000  0.682192  0.305128  0.469594  0.073795   \n",
       "DC        0.330512  0.682192  1.000000  0.229154  0.496208 -0.039192   \n",
       "ISI       0.531805  0.305128  0.229154  1.000000  0.394287 -0.132517   \n",
       "temp      0.431532  0.469594  0.496208  0.394287  1.000000 -0.527390   \n",
       "RH       -0.300995  0.073795 -0.039192 -0.132517 -0.527390  1.000000   \n",
       "wind     -0.028485 -0.105342 -0.203466  0.106826 -0.227116  0.069410   \n",
       "rain      0.056702  0.074790  0.035861  0.067668  0.069491  0.099751   \n",
       "area      0.040122  0.072994  0.049383  0.008258  0.097844 -0.075519   \n",
       "dayfri    0.019306 -0.012010 -0.004220  0.046695 -0.071949  0.064506   \n",
       "daymon   -0.059396 -0.107921 -0.052993 -0.158601 -0.136529  0.009376   \n",
       "daysat   -0.019637 -0.003653 -0.035189 -0.038585  0.034899 -0.023869   \n",
       "daysun   -0.089517  0.025355 -0.001431 -0.003243  0.014403  0.136220   \n",
       "daythu    0.071730  0.087672  0.051859 -0.022406  0.051432 -0.123061   \n",
       "daytue    0.011225  0.000016  0.028368  0.068610  0.035630 -0.014211   \n",
       "daywed    0.093908  0.017939  0.024803  0.125415  0.090580 -0.087508   \n",
       "monthapr -0.117199 -0.197543 -0.268211 -0.106478 -0.157051  0.021235   \n",
       "monthaug  0.228103  0.497928  0.279361  0.334639  0.351404  0.054761   \n",
       "monthdec -0.137044 -0.176301 -0.105642 -0.162322 -0.329648 -0.047714   \n",
       "monthfeb -0.281535 -0.317899 -0.399277 -0.249777 -0.320015  0.140430   \n",
       "monthjan -0.454771 -0.105647 -0.115064 -0.103588 -0.146520  0.170923   \n",
       "monthjul  0.031833 -0.001946 -0.100887  0.020982  0.142588  0.013185   \n",
       "monthjun -0.040634 -0.050403 -0.186183  0.111516  0.051015  0.009382   \n",
       "monthmar -0.074327 -0.407404 -0.650427 -0.143520 -0.341797 -0.089836   \n",
       "monthmay -0.037230 -0.081980 -0.114209 -0.060493 -0.045540  0.086822   \n",
       "monthnov -0.088964 -0.074218 -0.078380 -0.076559 -0.053798 -0.035885   \n",
       "monthoct -0.005998 -0.187632  0.093279 -0.071154 -0.053513 -0.072334   \n",
       "monthsep  0.076609  0.110907  0.531857 -0.068877  0.088006 -0.062596   \n",
       "\n",
       "              wind      rain      area    dayfri  ...  monthdec  monthfeb  \\\n",
       "FFMC     -0.028485  0.056702  0.040122  0.019306  ... -0.137044 -0.281535   \n",
       "DMC      -0.105342  0.074790  0.072994 -0.012010  ... -0.176301 -0.317899   \n",
       "DC       -0.203466  0.035861  0.049383 -0.004220  ... -0.105642 -0.399277   \n",
       "ISI       0.106826  0.067668  0.008258  0.046695  ... -0.162322 -0.249777   \n",
       "temp     -0.227116  0.069491  0.097844 -0.071949  ... -0.329648 -0.320015   \n",
       "RH        0.069410  0.099751 -0.075519  0.064506  ... -0.047714  0.140430   \n",
       "wind      1.000000  0.061119  0.012317  0.118090  ...  0.269702 -0.029431   \n",
       "rain      0.061119  1.000000 -0.007366 -0.004261  ... -0.009752 -0.014698   \n",
       "area      0.012317 -0.007366  1.000000 -0.052911  ...  0.001010 -0.020732   \n",
       "dayfri    0.118090 -0.004261 -0.052911  1.000000  ... -0.019140  0.046323   \n",
       "daymon   -0.063881 -0.029945 -0.021206 -0.181293  ...  0.114519  0.003933   \n",
       "daysat   -0.063799 -0.032271  0.087868 -0.195372  ... -0.058625  0.020406   \n",
       "daysun    0.027981 -0.017872 -0.020463 -0.210462  ... -0.024966  0.008416   \n",
       "daythu   -0.062553 -0.026798  0.020121 -0.162237  ... -0.002838 -0.042278   \n",
       "daytue    0.053396  0.139311 -0.001333 -0.166728  ... -0.005125 -0.014491   \n",
       "daywed   -0.019965 -0.020744 -0.011452 -0.151487  ...  0.002899 -0.035713   \n",
       "monthapr  0.048266 -0.009752 -0.008280 -0.019140  ... -0.017717 -0.026701   \n",
       "monthaug  0.028577  0.093101 -0.004187 -0.100837  ... -0.098941 -0.149116   \n",
       "monthdec  0.269702 -0.009752  0.001010 -0.019140  ...  1.000000 -0.026701   \n",
       "monthfeb -0.029431 -0.014698 -0.020732  0.046323  ... -0.026701  1.000000   \n",
       "monthjan -0.070245 -0.004566 -0.012589 -0.027643  ... -0.008295 -0.012501   \n",
       "monthjul -0.040645 -0.013390  0.006149 -0.048969  ... -0.034190 -0.051528   \n",
       "monthjun  0.012124 -0.013510 -0.020314  0.006000  ... -0.024543 -0.036989   \n",
       "monthmar  0.181433 -0.020744 -0.045596  0.036205  ... -0.045456 -0.068508   \n",
       "monthmay  0.015054 -0.004566  0.006264  0.056423  ... -0.008295 -0.012501   \n",
       "monthnov  0.011864 -0.003225 -0.008893 -0.019527  ... -0.005860 -0.008831   \n",
       "monthoct -0.053850 -0.012665 -0.016878 -0.045585  ... -0.023008 -0.034676   \n",
       "monthsep -0.181476 -0.051733  0.056573  0.107671  ... -0.093982 -0.141642   \n",
       "\n",
       "          monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  \\\n",
       "FFMC     -0.454771  0.031833 -0.040634 -0.074327 -0.037230 -0.088964   \n",
       "DMC      -0.105647 -0.001946 -0.050403 -0.407404 -0.081980 -0.074218   \n",
       "DC       -0.115064 -0.100887 -0.186183 -0.650427 -0.114209 -0.078380   \n",
       "ISI      -0.103588  0.020982  0.111516 -0.143520 -0.060493 -0.076559   \n",
       "temp     -0.146520  0.142588  0.051015 -0.341797 -0.045540 -0.053798   \n",
       "RH        0.170923  0.013185  0.009382 -0.089836  0.086822 -0.035885   \n",
       "wind     -0.070245 -0.040645  0.012124  0.181433  0.015054  0.011864   \n",
       "rain     -0.004566 -0.013390 -0.013510 -0.020744 -0.004566 -0.003225   \n",
       "area     -0.012589  0.006149 -0.020314 -0.045596  0.006264 -0.008893   \n",
       "dayfri   -0.027643 -0.048969  0.006000  0.036205  0.056423 -0.019527   \n",
       "daymon   -0.025470 -0.013300  0.017553  0.077125 -0.025470 -0.017992   \n",
       "daysat    0.057019  0.060945 -0.022408  0.021024  0.057019 -0.019390   \n",
       "daysun    0.050887 -0.018241  0.024540 -0.047726 -0.029568 -0.020887   \n",
       "daythu   -0.022793 -0.019300 -0.000195 -0.026885 -0.022793 -0.016101   \n",
       "daytue   -0.023424  0.049688 -0.069308 -0.032351 -0.023424  0.117121   \n",
       "daywed   -0.021282 -0.008985  0.043422 -0.033917 -0.021282 -0.015034   \n",
       "monthapr -0.008295 -0.034190 -0.024543 -0.045456 -0.008295 -0.005860   \n",
       "monthaug -0.046323 -0.190937 -0.137065 -0.253859 -0.046323 -0.032724   \n",
       "monthdec -0.008295 -0.034190 -0.024543 -0.045456 -0.008295 -0.005860   \n",
       "monthfeb -0.012501 -0.051528 -0.036989 -0.068508 -0.012501 -0.008831   \n",
       "monthjan  1.000000 -0.016007 -0.011491 -0.021282 -0.003883 -0.002743   \n",
       "monthjul -0.016007  1.000000 -0.047363 -0.087722 -0.016007 -0.011308   \n",
       "monthjun -0.011491 -0.047363  1.000000 -0.062972 -0.011491 -0.008117   \n",
       "monthmar -0.021282 -0.087722 -0.062972  1.000000 -0.021282 -0.015034   \n",
       "monthmay -0.003883 -0.016007 -0.011491 -0.021282  1.000000 -0.002743   \n",
       "monthnov -0.002743 -0.011308 -0.008117 -0.015034 -0.002743  1.000000   \n",
       "monthoct -0.010772 -0.044402 -0.031874 -0.059034 -0.010772 -0.007610   \n",
       "monthsep -0.044001 -0.181367 -0.130195 -0.241135 -0.044001 -0.031083   \n",
       "\n",
       "          monthoct  monthsep  \n",
       "FFMC     -0.005998  0.076609  \n",
       "DMC      -0.187632  0.110907  \n",
       "DC        0.093279  0.531857  \n",
       "ISI      -0.071154 -0.068877  \n",
       "temp     -0.053513  0.088006  \n",
       "RH       -0.072334 -0.062596  \n",
       "wind     -0.053850 -0.181476  \n",
       "rain     -0.012665 -0.051733  \n",
       "area     -0.016878  0.056573  \n",
       "dayfri   -0.045585  0.107671  \n",
       "daymon    0.060975  0.039632  \n",
       "daysat    0.017584 -0.032783  \n",
       "daysun    0.007252 -0.048817  \n",
       "daythu   -0.063223  0.008984  \n",
       "daytue    0.005008 -0.028570  \n",
       "daywed    0.016325 -0.053222  \n",
       "monthapr -0.023008 -0.093982  \n",
       "monthaug -0.128493 -0.524858  \n",
       "monthdec -0.023008 -0.093982  \n",
       "monthfeb -0.034676 -0.141642  \n",
       "monthjan -0.010772 -0.044001  \n",
       "monthjul -0.044402 -0.181367  \n",
       "monthjun -0.031874 -0.130195  \n",
       "monthmar -0.059034 -0.241135  \n",
       "monthmay -0.010772 -0.044001  \n",
       "monthnov -0.007610 -0.031083  \n",
       "monthoct  1.000000 -0.122053  \n",
       "monthsep -0.122053  1.000000  \n",
       "\n",
       "[28 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04834985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "data['size_category']= label_encoder.fit_transform(data['size_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3550720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FFMC    DMC     DC   ISI  temp  RH  wind  rain   area  dayfri  ...  \\\n",
       "0    86.2   26.2   94.3   5.1   8.2  51   6.7   0.0   0.00       1  ...   \n",
       "1    90.6   35.4  669.1   6.7  18.0  33   0.9   0.0   0.00       0  ...   \n",
       "2    90.6   43.7  686.9   6.7  14.6  33   1.3   0.0   0.00       0  ...   \n",
       "3    91.7   33.3   77.5   9.0   8.3  97   4.0   0.2   0.00       1  ...   \n",
       "4    89.3   51.3  102.2   9.6  11.4  99   1.8   0.0   0.00       0  ...   \n",
       "..    ...    ...    ...   ...   ...  ..   ...   ...    ...     ...  ...   \n",
       "512  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0   6.44       0  ...   \n",
       "513  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  54.29       0  ...   \n",
       "514  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  11.16       0  ...   \n",
       "515  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0   0.00       0  ...   \n",
       "516  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0   0.00       0  ...   \n",
       "\n",
       "     monthfeb  monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  \\\n",
       "0           0         0         0         0         1         0         0   \n",
       "1           0         0         0         0         0         0         0   \n",
       "2           0         0         0         0         0         0         0   \n",
       "3           0         0         0         0         1         0         0   \n",
       "4           0         0         0         0         1         0         0   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512         0         0         0         0         0         0         0   \n",
       "513         0         0         0         0         0         0         0   \n",
       "514         0         0         0         0         0         0         0   \n",
       "515         0         0         0         0         0         0         0   \n",
       "516         0         0         0         0         0         0         1   \n",
       "\n",
       "     monthoct  monthsep  size_category  \n",
       "0           0         0              1  \n",
       "1           1         0              1  \n",
       "2           1         0              1  \n",
       "3           0         0              1  \n",
       "4           0         0              1  \n",
       "..        ...       ...            ...  \n",
       "512         0         0              0  \n",
       "513         0         0              0  \n",
       "514         0         0              0  \n",
       "515         0         0              1  \n",
       "516         0         0              1  \n",
       "\n",
       "[517 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.drop(columns=['month','day'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461afe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 29 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   FFMC           517 non-null    float64\n",
      " 1   DMC            517 non-null    float64\n",
      " 2   DC             517 non-null    float64\n",
      " 3   ISI            517 non-null    float64\n",
      " 4   temp           517 non-null    float64\n",
      " 5   RH             517 non-null    int64  \n",
      " 6   wind           517 non-null    float64\n",
      " 7   rain           517 non-null    float64\n",
      " 8   area           517 non-null    float64\n",
      " 9   dayfri         517 non-null    int64  \n",
      " 10  daymon         517 non-null    int64  \n",
      " 11  daysat         517 non-null    int64  \n",
      " 12  daysun         517 non-null    int64  \n",
      " 13  daythu         517 non-null    int64  \n",
      " 14  daytue         517 non-null    int64  \n",
      " 15  daywed         517 non-null    int64  \n",
      " 16  monthapr       517 non-null    int64  \n",
      " 17  monthaug       517 non-null    int64  \n",
      " 18  monthdec       517 non-null    int64  \n",
      " 19  monthfeb       517 non-null    int64  \n",
      " 20  monthjan       517 non-null    int64  \n",
      " 21  monthjul       517 non-null    int64  \n",
      " 22  monthjun       517 non-null    int64  \n",
      " 23  monthmar       517 non-null    int64  \n",
      " 24  monthmay       517 non-null    int64  \n",
      " 25  monthnov       517 non-null    int64  \n",
      " 26  monthoct       517 non-null    int64  \n",
      " 27  monthsep       517 non-null    int64  \n",
      " 28  size_category  517 non-null    int32  \n",
      "dtypes: float64(8), int32(1), int64(20)\n",
      "memory usage: 115.2 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c953a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,0:-1]\n",
    "y = data['size_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89534a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthdec</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FFMC    DMC     DC   ISI  temp  RH  wind  rain   area  dayfri  ...  \\\n",
       "0    86.2   26.2   94.3   5.1   8.2  51   6.7   0.0   0.00       1  ...   \n",
       "1    90.6   35.4  669.1   6.7  18.0  33   0.9   0.0   0.00       0  ...   \n",
       "2    90.6   43.7  686.9   6.7  14.6  33   1.3   0.0   0.00       0  ...   \n",
       "3    91.7   33.3   77.5   9.0   8.3  97   4.0   0.2   0.00       1  ...   \n",
       "4    89.3   51.3  102.2   9.6  11.4  99   1.8   0.0   0.00       0  ...   \n",
       "..    ...    ...    ...   ...   ...  ..   ...   ...    ...     ...  ...   \n",
       "512  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0   6.44       0  ...   \n",
       "513  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  54.29       0  ...   \n",
       "514  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  11.16       0  ...   \n",
       "515  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0   0.00       0  ...   \n",
       "516  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0   0.00       0  ...   \n",
       "\n",
       "     monthdec  monthfeb  monthjan  monthjul  monthjun  monthmar  monthmay  \\\n",
       "0           0         0         0         0         0         1         0   \n",
       "1           0         0         0         0         0         0         0   \n",
       "2           0         0         0         0         0         0         0   \n",
       "3           0         0         0         0         0         1         0   \n",
       "4           0         0         0         0         0         1         0   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512         0         0         0         0         0         0         0   \n",
       "513         0         0         0         0         0         0         0   \n",
       "514         0         0         0         0         0         0         0   \n",
       "515         0         0         0         0         0         0         0   \n",
       "516         0         0         0         0         0         0         0   \n",
       "\n",
       "     monthnov  monthoct  monthsep  \n",
       "0           0         0         0  \n",
       "1           0         1         0  \n",
       "2           0         1         0  \n",
       "3           0         0         0  \n",
       "4           0         0         0  \n",
       "..        ...       ...       ...  \n",
       "512         0         0         0  \n",
       "513         0         0         0  \n",
       "514         0         0         0  \n",
       "515         0         0         0  \n",
       "516         1         0         0  \n",
       "\n",
       "[517 rows x 28 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1671edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "512    0\n",
       "513    0\n",
       "514    0\n",
       "515    1\n",
       "516    1\n",
       "Name: size_category, Length: 517, dtype: int32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8422b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a505679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Artificial Neural Network Model - Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c59636f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(42, input_dim=28, activation='relu'))\n",
    "model.add(Dense(28, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d9b0657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "201001c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "28/28 [==============================] - 1s 10ms/step - loss: 8.6412 - accuracy: 0.6159 - val_loss: 3.2718 - val_accuracy: 0.7372\n",
      "Epoch 2/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2.4987 - accuracy: 0.6087 - val_loss: 0.6413 - val_accuracy: 0.7883\n",
      "Epoch 3/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.9048 - accuracy: 0.6920 - val_loss: 0.4104 - val_accuracy: 0.7956\n",
      "Epoch 4/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5413 - accuracy: 0.7246 - val_loss: 0.3452 - val_accuracy: 0.9124\n",
      "Epoch 5/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5521 - accuracy: 0.7609 - val_loss: 0.2936 - val_accuracy: 0.8905\n",
      "Epoch 6/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4728 - accuracy: 0.8225 - val_loss: 0.4559 - val_accuracy: 0.7956\n",
      "Epoch 7/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3600 - accuracy: 0.8514 - val_loss: 0.2542 - val_accuracy: 0.9270\n",
      "Epoch 8/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3473 - accuracy: 0.8514 - val_loss: 0.2633 - val_accuracy: 0.9197\n",
      "Epoch 9/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8551 - val_loss: 0.2444 - val_accuracy: 0.9343\n",
      "Epoch 10/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3503 - accuracy: 0.8659 - val_loss: 0.3064 - val_accuracy: 0.8978\n",
      "Epoch 11/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3645 - accuracy: 0.8370 - val_loss: 0.3921 - val_accuracy: 0.8832\n",
      "Epoch 12/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2943 - accuracy: 0.8877 - val_loss: 0.2138 - val_accuracy: 0.9270\n",
      "Epoch 13/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2683 - accuracy: 0.8877 - val_loss: 0.2158 - val_accuracy: 0.9197\n",
      "Epoch 14/180\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2406 - accuracy: 0.9058 - val_loss: 0.1877 - val_accuracy: 0.9124\n",
      "Epoch 15/180\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2745 - accuracy: 0.8696 - val_loss: 0.3787 - val_accuracy: 0.8905\n",
      "Epoch 16/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8841 - val_loss: 0.3646 - val_accuracy: 0.8832\n",
      "Epoch 17/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2850 - accuracy: 0.8841 - val_loss: 0.1995 - val_accuracy: 0.9124\n",
      "Epoch 18/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2873 - accuracy: 0.8841 - val_loss: 0.1762 - val_accuracy: 0.9124\n",
      "Epoch 19/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.9348 - val_loss: 0.1634 - val_accuracy: 0.9124\n",
      "Epoch 20/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2213 - accuracy: 0.9167 - val_loss: 0.1578 - val_accuracy: 0.9197\n",
      "Epoch 21/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2137 - accuracy: 0.9312 - val_loss: 0.3481 - val_accuracy: 0.8978\n",
      "Epoch 22/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2772 - accuracy: 0.9167 - val_loss: 0.2055 - val_accuracy: 0.9708\n",
      "Epoch 23/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2189 - accuracy: 0.9420 - val_loss: 0.2246 - val_accuracy: 0.9124\n",
      "Epoch 24/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2652 - accuracy: 0.9022 - val_loss: 0.1582 - val_accuracy: 0.9708\n",
      "Epoch 25/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1982 - accuracy: 0.9493 - val_loss: 0.1603 - val_accuracy: 0.9197\n",
      "Epoch 26/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9275 - val_loss: 0.1310 - val_accuracy: 0.9197\n",
      "Epoch 27/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1669 - accuracy: 0.9275 - val_loss: 0.2490 - val_accuracy: 0.9270\n",
      "Epoch 28/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9239 - val_loss: 0.1343 - val_accuracy: 0.9124\n",
      "Epoch 29/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.9420 - val_loss: 0.1237 - val_accuracy: 0.9124\n",
      "Epoch 30/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9275 - val_loss: 0.1205 - val_accuracy: 0.9270\n",
      "Epoch 31/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1483 - accuracy: 0.9565 - val_loss: 0.1454 - val_accuracy: 0.9270\n",
      "Epoch 32/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1383 - accuracy: 0.9420 - val_loss: 0.1112 - val_accuracy: 0.9343\n",
      "Epoch 33/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1165 - accuracy: 0.9674 - val_loss: 0.1619 - val_accuracy: 0.9197\n",
      "Epoch 34/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1993 - accuracy: 0.9312 - val_loss: 0.1062 - val_accuracy: 0.9781\n",
      "Epoch 35/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1736 - accuracy: 0.9239 - val_loss: 0.2633 - val_accuracy: 0.9051\n",
      "Epoch 36/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1310 - accuracy: 0.9493 - val_loss: 0.1114 - val_accuracy: 0.9343\n",
      "Epoch 37/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2013 - accuracy: 0.9312 - val_loss: 0.1393 - val_accuracy: 0.9124\n",
      "Epoch 38/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2448 - accuracy: 0.9058 - val_loss: 0.1191 - val_accuracy: 0.9708\n",
      "Epoch 39/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.9493 - val_loss: 0.0859 - val_accuracy: 0.9489\n",
      "Epoch 40/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.9674 - val_loss: 0.1203 - val_accuracy: 0.9270\n",
      "Epoch 41/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1741 - accuracy: 0.9457 - val_loss: 0.0914 - val_accuracy: 0.9489\n",
      "Epoch 42/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.9601 - val_loss: 0.0962 - val_accuracy: 0.9416\n",
      "Epoch 43/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0893 - accuracy: 0.9855 - val_loss: 0.0905 - val_accuracy: 0.9416\n",
      "Epoch 44/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1550 - accuracy: 0.9348 - val_loss: 0.0875 - val_accuracy: 0.9854\n",
      "Epoch 45/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1485 - accuracy: 0.9565 - val_loss: 0.0837 - val_accuracy: 0.9562\n",
      "Epoch 46/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0967 - accuracy: 0.9674 - val_loss: 0.0733 - val_accuracy: 0.9708\n",
      "Epoch 47/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.9746 - val_loss: 0.0755 - val_accuracy: 0.9781\n",
      "Epoch 48/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1236 - accuracy: 0.9529 - val_loss: 0.0689 - val_accuracy: 0.9854\n",
      "Epoch 49/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1101 - accuracy: 0.9638 - val_loss: 0.0697 - val_accuracy: 0.9781\n",
      "Epoch 50/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.9746 - val_loss: 0.0982 - val_accuracy: 0.9781\n",
      "Epoch 51/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.9565 - val_loss: 0.0729 - val_accuracy: 0.9562\n",
      "Epoch 52/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0888 - accuracy: 0.9638 - val_loss: 0.0642 - val_accuracy: 0.9781\n",
      "Epoch 53/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0712 - accuracy: 0.9746 - val_loss: 0.0626 - val_accuracy: 0.9854\n",
      "Epoch 54/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.9638 - val_loss: 0.1071 - val_accuracy: 0.9708\n",
      "Epoch 55/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0956 - accuracy: 0.9638 - val_loss: 0.1244 - val_accuracy: 0.9562\n",
      "Epoch 56/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0853 - accuracy: 0.9674 - val_loss: 0.0774 - val_accuracy: 0.9708\n",
      "Epoch 57/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0833 - accuracy: 0.9601 - val_loss: 0.0905 - val_accuracy: 0.9708\n",
      "Epoch 58/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1963 - accuracy: 0.9130 - val_loss: 0.3069 - val_accuracy: 0.9124\n",
      "Epoch 59/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4241 - accuracy: 0.8841 - val_loss: 0.2667 - val_accuracy: 0.9051\n",
      "Epoch 60/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2019 - accuracy: 0.9239 - val_loss: 0.0831 - val_accuracy: 0.9489\n",
      "Epoch 61/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1315 - accuracy: 0.9565 - val_loss: 0.1297 - val_accuracy: 0.9416\n",
      "Epoch 62/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1226 - accuracy: 0.9565 - val_loss: 0.1901 - val_accuracy: 0.9270\n",
      "Epoch 63/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.9674 - val_loss: 0.0616 - val_accuracy: 0.9708\n",
      "Epoch 64/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0843 - accuracy: 0.9710 - val_loss: 0.0664 - val_accuracy: 0.9489\n",
      "Epoch 65/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9746 - val_loss: 0.1385 - val_accuracy: 0.9489\n",
      "Epoch 66/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0857 - accuracy: 0.9710 - val_loss: 0.0800 - val_accuracy: 0.9416\n",
      "Epoch 67/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9638 - val_loss: 0.0595 - val_accuracy: 0.9708\n",
      "Epoch 68/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1677 - accuracy: 0.9457 - val_loss: 0.0667 - val_accuracy: 0.9708\n",
      "Epoch 69/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.9674 - val_loss: 0.0489 - val_accuracy: 0.9854\n",
      "Epoch 70/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9746 - val_loss: 0.0555 - val_accuracy: 0.9854\n",
      "Epoch 71/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9819 - val_loss: 0.0508 - val_accuracy: 0.9854\n",
      "Epoch 72/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9891 - val_loss: 0.0531 - val_accuracy: 0.9854\n",
      "Epoch 73/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.9746 - val_loss: 0.0777 - val_accuracy: 0.9489\n",
      "Epoch 74/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1216 - accuracy: 0.9384 - val_loss: 0.0980 - val_accuracy: 0.9635\n",
      "Epoch 75/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.9601 - val_loss: 0.0673 - val_accuracy: 0.9635\n",
      "Epoch 76/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.9674 - val_loss: 0.4689 - val_accuracy: 0.9051\n",
      "Epoch 77/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9457 - val_loss: 0.0598 - val_accuracy: 0.9781\n",
      "Epoch 78/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1096 - accuracy: 0.9529 - val_loss: 0.0576 - val_accuracy: 0.9781\n",
      "Epoch 79/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.9783 - val_loss: 0.0512 - val_accuracy: 0.9781\n",
      "Epoch 80/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9746 - val_loss: 0.0558 - val_accuracy: 0.9854\n",
      "Epoch 81/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1150 - accuracy: 0.9493 - val_loss: 0.2163 - val_accuracy: 0.9197\n",
      "Epoch 82/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1827 - accuracy: 0.9348 - val_loss: 0.0530 - val_accuracy: 0.9854\n",
      "Epoch 83/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9855 - val_loss: 0.0561 - val_accuracy: 0.9781\n",
      "Epoch 84/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9855 - val_loss: 0.0902 - val_accuracy: 0.9635\n",
      "Epoch 85/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1961 - accuracy: 0.9348 - val_loss: 0.1545 - val_accuracy: 0.9416\n",
      "Epoch 86/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.9638 - val_loss: 0.0567 - val_accuracy: 0.9708\n",
      "Epoch 87/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9819 - val_loss: 0.0523 - val_accuracy: 0.9854\n",
      "Epoch 88/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9891 - val_loss: 0.0897 - val_accuracy: 0.9708\n",
      "Epoch 89/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1113 - accuracy: 0.9565 - val_loss: 0.1348 - val_accuracy: 0.9343\n",
      "Epoch 90/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9891 - val_loss: 0.1436 - val_accuracy: 0.9270\n",
      "Epoch 91/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1823 - accuracy: 0.9384 - val_loss: 0.0605 - val_accuracy: 0.9854\n",
      "Epoch 92/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9746 - val_loss: 0.0556 - val_accuracy: 0.9854\n",
      "Epoch 93/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0356 - accuracy: 0.9928 - val_loss: 0.0480 - val_accuracy: 0.9781\n",
      "Epoch 94/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0364 - accuracy: 0.9891 - val_loss: 0.0482 - val_accuracy: 0.9854\n",
      "Epoch 95/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9855 - val_loss: 0.0484 - val_accuracy: 0.9781\n",
      "Epoch 96/180\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.9783 - val_loss: 0.0543 - val_accuracy: 0.9854\n",
      "Epoch 97/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1151 - accuracy: 0.9565 - val_loss: 0.2602 - val_accuracy: 0.9270\n",
      "Epoch 98/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9783 - val_loss: 0.0489 - val_accuracy: 0.9854\n",
      "Epoch 99/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9891 - val_loss: 0.0505 - val_accuracy: 0.9854\n",
      "Epoch 100/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 0.9855 - val_loss: 0.1216 - val_accuracy: 0.9489\n",
      "Epoch 101/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9710 - val_loss: 0.1366 - val_accuracy: 0.9489\n",
      "Epoch 102/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.9601 - val_loss: 0.0542 - val_accuracy: 0.9781\n",
      "Epoch 103/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9783 - val_loss: 0.0487 - val_accuracy: 0.9781\n",
      "Epoch 104/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9746 - val_loss: 0.1238 - val_accuracy: 0.9416\n",
      "Epoch 105/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0930 - accuracy: 0.9638 - val_loss: 0.3220 - val_accuracy: 0.9270\n",
      "Epoch 106/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1649 - accuracy: 0.9384 - val_loss: 0.6403 - val_accuracy: 0.9051\n",
      "Epoch 107/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.9130 - val_loss: 0.0700 - val_accuracy: 0.9708\n",
      "Epoch 108/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1312 - accuracy: 0.9529 - val_loss: 0.0521 - val_accuracy: 0.9708\n",
      "Epoch 109/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0894 - accuracy: 0.9601 - val_loss: 0.0555 - val_accuracy: 0.9781\n",
      "Epoch 110/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0432 - accuracy: 0.9855 - val_loss: 0.0518 - val_accuracy: 0.9708\n",
      "Epoch 111/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9783 - val_loss: 0.0826 - val_accuracy: 0.9416\n",
      "Epoch 112/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1261 - accuracy: 0.9601 - val_loss: 0.1291 - val_accuracy: 0.9489\n",
      "Epoch 113/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0821 - accuracy: 0.9674 - val_loss: 0.1030 - val_accuracy: 0.9562\n",
      "Epoch 114/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0587 - accuracy: 0.9819 - val_loss: 0.1875 - val_accuracy: 0.9270\n",
      "Epoch 115/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0497 - accuracy: 0.9855 - val_loss: 0.0614 - val_accuracy: 0.9562\n",
      "Epoch 116/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0380 - accuracy: 0.9855 - val_loss: 0.0664 - val_accuracy: 0.9562\n",
      "Epoch 117/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0617 - accuracy: 0.9710 - val_loss: 0.0543 - val_accuracy: 0.9781\n",
      "Epoch 118/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0456 - accuracy: 0.9746 - val_loss: 0.0533 - val_accuracy: 0.9781\n",
      "Epoch 119/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9819 - val_loss: 0.0638 - val_accuracy: 0.9635\n",
      "Epoch 120/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0853 - accuracy: 0.9746 - val_loss: 0.0577 - val_accuracy: 0.9708\n",
      "Epoch 121/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0905 - accuracy: 0.9638 - val_loss: 0.1831 - val_accuracy: 0.9343\n",
      "Epoch 122/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.9746 - val_loss: 0.0512 - val_accuracy: 0.9781\n",
      "Epoch 123/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0378 - accuracy: 0.9819 - val_loss: 0.0654 - val_accuracy: 0.9562\n",
      "Epoch 124/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0851 - accuracy: 0.9638 - val_loss: 0.1055 - val_accuracy: 0.9635\n",
      "Epoch 125/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1041 - accuracy: 0.9674 - val_loss: 0.0676 - val_accuracy: 0.9562\n",
      "Epoch 126/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9710 - val_loss: 0.0859 - val_accuracy: 0.9635\n",
      "Epoch 127/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9710 - val_loss: 0.1424 - val_accuracy: 0.9270\n",
      "Epoch 128/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0324 - accuracy: 0.9819 - val_loss: 0.0553 - val_accuracy: 0.9635\n",
      "Epoch 129/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.9891 - val_loss: 0.0493 - val_accuracy: 0.9854\n",
      "Epoch 130/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.9928 - val_loss: 0.1055 - val_accuracy: 0.9489\n",
      "Epoch 131/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9819 - val_loss: 0.0553 - val_accuracy: 0.9708\n",
      "Epoch 132/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 0.9783 - val_loss: 0.1667 - val_accuracy: 0.9343\n",
      "Epoch 133/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0489 - accuracy: 0.9783 - val_loss: 0.0709 - val_accuracy: 0.9635\n",
      "Epoch 134/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0333 - accuracy: 0.9891 - val_loss: 0.2093 - val_accuracy: 0.9270\n",
      "Epoch 135/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.9493 - val_loss: 0.0646 - val_accuracy: 0.9635\n",
      "Epoch 136/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.9855 - val_loss: 0.0618 - val_accuracy: 0.9635\n",
      "Epoch 137/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.9855 - val_loss: 0.0509 - val_accuracy: 0.9708\n",
      "Epoch 138/180\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0320 - accuracy: 0.9891 - val_loss: 0.0519 - val_accuracy: 0.9781\n",
      "Epoch 139/180\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 0.9928 - val_loss: 0.0570 - val_accuracy: 0.9708\n",
      "Epoch 140/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9674 - val_loss: 0.0808 - val_accuracy: 0.9635\n",
      "Epoch 141/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0423 - accuracy: 0.9855 - val_loss: 0.0545 - val_accuracy: 0.9708\n",
      "Epoch 142/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0476 - accuracy: 0.9855 - val_loss: 0.0920 - val_accuracy: 0.9489\n",
      "Epoch 143/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 0.9855 - val_loss: 0.0591 - val_accuracy: 0.9708\n",
      "Epoch 144/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.9638 - val_loss: 0.0647 - val_accuracy: 0.9562\n",
      "Epoch 145/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2685 - accuracy: 0.9457 - val_loss: 0.0666 - val_accuracy: 0.9781\n",
      "Epoch 146/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2837 - accuracy: 0.9457 - val_loss: 0.0532 - val_accuracy: 0.9781\n",
      "Epoch 147/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9855 - val_loss: 0.0688 - val_accuracy: 0.9635\n",
      "Epoch 148/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9674 - val_loss: 0.1110 - val_accuracy: 0.9489\n",
      "Epoch 149/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9746 - val_loss: 0.0489 - val_accuracy: 0.9854\n",
      "Epoch 150/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.9783 - val_loss: 0.0531 - val_accuracy: 0.9781\n",
      "Epoch 151/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0361 - accuracy: 0.9891 - val_loss: 0.2135 - val_accuracy: 0.9343\n",
      "Epoch 152/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0426 - accuracy: 0.9855 - val_loss: 0.2045 - val_accuracy: 0.9416\n",
      "Epoch 153/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.9638 - val_loss: 0.2430 - val_accuracy: 0.9197\n",
      "Epoch 154/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9710 - val_loss: 0.0558 - val_accuracy: 0.9708\n",
      "Epoch 155/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0297 - accuracy: 0.9855 - val_loss: 0.0518 - val_accuracy: 0.9781\n",
      "Epoch 156/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.9891 - val_loss: 0.0567 - val_accuracy: 0.9708\n",
      "Epoch 157/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0429 - accuracy: 0.9746 - val_loss: 0.0542 - val_accuracy: 0.9708\n",
      "Epoch 158/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9746 - val_loss: 0.0554 - val_accuracy: 0.9708\n",
      "Epoch 159/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.9819 - val_loss: 0.1245 - val_accuracy: 0.9489\n",
      "Epoch 160/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0434 - accuracy: 0.9891 - val_loss: 0.0508 - val_accuracy: 0.9781\n",
      "Epoch 161/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9855 - val_loss: 0.0465 - val_accuracy: 0.9854\n",
      "Epoch 162/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9746 - val_loss: 0.1288 - val_accuracy: 0.9489\n",
      "Epoch 163/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0792 - accuracy: 0.9819 - val_loss: 0.0580 - val_accuracy: 0.9708\n",
      "Epoch 164/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0485 - accuracy: 0.9819 - val_loss: 0.0596 - val_accuracy: 0.9708\n",
      "Epoch 165/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.9928 - val_loss: 0.0498 - val_accuracy: 0.9781\n",
      "Epoch 166/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.9855 - val_loss: 0.0592 - val_accuracy: 0.9708\n",
      "Epoch 167/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9819 - val_loss: 0.0653 - val_accuracy: 0.9708\n",
      "Epoch 168/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9746 - val_loss: 0.1508 - val_accuracy: 0.9343\n",
      "Epoch 169/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1008 - accuracy: 0.9674 - val_loss: 0.1013 - val_accuracy: 0.9635\n",
      "Epoch 170/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.9710 - val_loss: 0.0497 - val_accuracy: 0.9854\n",
      "Epoch 171/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.9819 - val_loss: 0.3680 - val_accuracy: 0.9270\n",
      "Epoch 172/180\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9746 - val_loss: 0.0617 - val_accuracy: 0.9635\n",
      "Epoch 173/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.9891 - val_loss: 0.2091 - val_accuracy: 0.9343\n",
      "Epoch 174/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9710 - val_loss: 0.1367 - val_accuracy: 0.9416\n",
      "Epoch 175/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0389 - accuracy: 0.9746 - val_loss: 0.0614 - val_accuracy: 0.9708\n",
      "Epoch 176/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9855 - val_loss: 0.0478 - val_accuracy: 0.9781\n",
      "Epoch 177/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9783 - val_loss: 0.0690 - val_accuracy: 0.9708\n",
      "Epoch 178/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0813 - accuracy: 0.9638 - val_loss: 0.1498 - val_accuracy: 0.9416\n",
      "Epoch 179/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1246 - accuracy: 0.9565 - val_loss: 0.1075 - val_accuracy: 0.9562\n",
      "Epoch 180/180\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0426 - accuracy: 0.9891 - val_loss: 0.0428 - val_accuracy: 0.9854\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history=model.fit(x_train,y_train, validation_split=0.33, epochs=180, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c06ced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0341 - accuracy: 0.9826\n",
      "accuracy: 98.26%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(x, y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd8e844b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize training history\n",
    "\n",
    "# list all data in history\n",
    "history.history.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "156bf748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABpWUlEQVR4nO2dZ3hcV7Ww3zWjmZFGvduW5e64JE4ct/RGekJoCZAGIUBCLi1wCR0ufJQLXMrlAimEEEJNSCEhJE4nPU5c4t5tuUiyrC6Nprf9/dhniqSRPHY8km3t93n0aOacs89Z58zMXnuVvbYopTAYDAaDYSC20RbAYDAYDEcmRkEYDAaDISNGQRgMBoMhI0ZBGAwGgyEjRkEYDAaDISNGQRgMBoMhI0ZBGAyAiNwnIj/I8tjdInJBrmUyGEYboyAMBoPBkBGjIAyGYwgRyRttGQzHDkZBGI4aLNfOl0VknYj4ROT3IlIrIk+JSJ+IPC8i5WnHv0dENopIj4i8JCJz0vadLCJvW+3+DuQPuNa7RWSN1fYNETkxSxkvF5HVIuIRkUYR+e6A/Wda5+ux9n/M2l4gIj8XkT0i0isir1nbzhWRpgzP4QLr9XdF5GER+YuIeICPicgSEVlmXaNFRH4jIs609seLyHMi0iUirSLyDREZJyJ+EalMO26hiLSLiCObezccexgFYTjauBK4EDgOuAJ4CvgGUIX+Pn8eQESOA+4HvgBUA0uBf4mI0+osHwP+DFQAD1nnxWq7ALgX+BRQCfwWeFxEXFnI5wM+CpQBlwP/ISLvs847yZL315ZM84E1VrufAQuB0y2ZvgLEs3wm7wUetq75VyAGfBH9TE4Dzgc+bclQDDwPPA1MAGYALyil9gMvAR9KO+/1wANKqUiWchiOMYyCMBxt/Fop1aqUagZeBd5SSq1WSoWAR4GTreM+DDyplHrO6uB+BhSgO+BTAQfwS6VURCn1MLAi7Ro3Ab9VSr2llIoppf4IhKx2w6KUekkptV4pFVdKrUMrqXOs3dcBzyul7reu26mUWiMiNuDjwK1KqWbrmm9Y95QNy5RSj1nXDCilViml3lRKRZVSu9EKLiHDu4H9SqmfK6WCSqk+pdRb1r4/opUCImIHrkErUcMYxSgIw9FGa9rrQIb3RdbrCcCexA6lVBxoBOqsfc2qf6XKPWmvJwNfslw0PSLSA9Rb7YZFRE4RkRct10wvcAt6JI91jp0ZmlWhXVyZ9mVD4wAZjhORJ0Rkv+V2+u8sZAD4JzBXRKahrbRepdTyQ5TJcAxgFIThWGUfuqMHQEQE3Tk2Ay1AnbUtwaS0143AD5VSZWl/bqXU/Vlc92/A40C9UqoUuAtIXKcRmJ6hTQcQHGKfD3Cn3Ycd7Z5KZ2BJ5juBLcBMpVQJ2gV3IBlQSgWBB9GWzkcw1sOYxygIw7HKg8DlInK+FWT9EtpN9AawDIgCnxeRPBH5ALAkre3vgFssa0BEpNAKPhdncd1ioEspFRSRJcC1afv+ClwgIh+yrlspIvMt6+Ze4BciMkFE7CJymhXz2AbkW9d3AN8CDhQLKQY8gFdEZgP/kbbvCWCciHxBRFwiUiwip6Tt/xPwMeA9wF+yuF/DMYxREIZjEqXUVrQ//dfoEfoVwBVKqbBSKgx8AN0RdqPjFf9Ia7sSHYf4jbV/h3VsNnwa+J6I9AH/hVZUifPuBS5DK6sudID6JGv3bcB6dCykC/gJYFNK9VrnvAdt/fiAfllNGbgNrZj60Mru72ky9KHdR1cA+4HtwHlp+19HB8fftuIXhjGMmAWDDAZDOiLyb+BvSql7RlsWw+hiFITBYEgiIouB59AxlL7RlscwuhgXk8FgAEBE/oieI/EFoxwMYCwIg8FgMAyBsSAMBoPBkJFjqrBXVVWVmjJlymiLYTAYDEcNq1at6lBKDZxbAxxjCmLKlCmsXLlytMUwGAyGowYR2TPUPuNiMhgMBkNGcqYgROReEWkTkQ1D7BcR+ZWI7BBdvnlB2r5LRGSrte9ruZLRYDAYDEOTSwviPuCSYfZfCsy0/m5G149J1Jq53do/F7hGRObmUE6DwWAwZCBnMQil1CsiMmWYQ94L/MmqqPmmiJSJyHhgCrBDKdUAICIPWMduOhQ5IpEITU1NBIPBQ2l+1JCfn8/EiRNxOMzaLgaD4fAwmkHqOvqXKW6ytmXanl5M7KBoamqiuLiYKVOm0L9457GDUorOzk6ampqYOnXqaItjMBiOEUYzSJ2pt1bDbM98EpGbRWSliKxsb28ftD8YDFJZWXnMKgcAEaGysvKYt5IMBsPIMpoKogldnz/BRHQN/6G2Z0QpdbdSapFSalF1dcZU3mNaOSQYC/doMBhGltFUEI8DH7WymU5Fr17Vgi53PFNEplprB19tHWswHBM0dvl5bHUzY7HMzbbWPl7f0XFIbXv9Ef7y5h5i8aPjuSmleHBFI92+8GiLcsjkMs31fvTCLLNEpElEPiEit4jILdYhS4EGdK3932Etqq6UigKfBZ4BNgMPKqU25krOXNPT08Mdd9xx0O0uu+wyenp6Dr9AhlHn7lca+MLf13DHS4e6wujRyeYWD1fe+QYfv28FgXDsoNoGwjFuvG8533psA6v3dudIwsPL2qZevvLIOn727NbRFuWQyZmCUEpdo5Qar5RyKKUmKqV+r5S6Syl1l7VfKaU+o5SarpSaZy3Skmi7VCl1nLXvh7mScSQYSkHEYsP/QJYuXUpZWVmOpBpbeIIRfrR0M55gJOfXUkrxm39vZ0ebF4CVu7t4ZFX/9X02tXgQgZ8+s5UHlu8d8lzdvjC/eHYrvf7cy51rWj1BPnrvcqIxRSgaz2hF/HNNM69uHxxHjMTifPqvq3h7bw8Ae7v8h02uB5bvHdai2dbax/88vSWj1bJqTxe/f23XkJbg85v0cukPr2qiwxvKWqZtrX3c/Ur2g4cHVzTyrcfWZ338wWBmUueYr33ta+zcuZP58+ezePFizjvvPK699lrmzZsHwPve9z4WLlzI8ccfz913351sN2XKFDo6Oti9ezdz5szhpptu4vjjj+eiiy4iEAiM1u0cldz3+m5++0oDz2zYn/NrvbytnZ89u41HV2ul8PvXdvHjp7ck98fjiq37+7hmySTOOa6abzy6nmc2Zpbrb8v38qt/7+ATfzz4EfeRxp0v7aTHH+ahW06j2JXH85tb++0PR+N889EN3P1KQ7/t8bjiqw+v48Wt7fzXu/V0qMauw/P97/aF+caj67nxDytYtrNz0P7GLj/X3fMWd7y0k437egfJ9eWH1vH9JzYNaSE8v7mVKZVuQtE4f1o2ZDWLQdz72i7+e+kW2vuyUypv7Ozgpa2DFevh4JiqxXQg/t+/NrJpn+ewnnPuhBK+c8XxQ+7/8Y9/zIYNG1izZg0vvfQSl19+ORs2bEimo957771UVFQQCARYvHgxV155JZWVlf3OsX37du6//35+97vf8aEPfYhHHnmE66+//rDex1C8saODl7a1842q16B0Isy6tP8B8Rg89RWYfy3ULcx8kl2vwM4X4YLv5F7g7j3w1Fchpn9cMaVY3NDFXQ4nK3Z+GxbVH+AEaWx5Eroa4PTPQagPnv4ay6d+hp+83kNcKa4/ZTJXLpyYOn7t39n24iZgCW0eff1WT5DeQAT+/UNoXkkw7sAdeg/z6ubw3ap/8+tuL5+738aTH65kZuNDcMlPQASWfpnT16zi7oJ8bt17E195ZB2/vubkQ34sT29oYW1TL1+9ZHbG/W19Qb756AY6vCFqi/P51TUn48w7POPHHn+YF1as50/VT3BCzXmcM6ua5ze3EY8rbDadXPHWrk68oSidXu2v37TPw3cf34gnGGHL/j6+eMFxfPzMqfz2lZ00dmsL4vtPbOLtNHfT9OoifnrViYMSNh5e1URvIMInzpwKfa3w7+/DRT/gxa1e4grK3A5u+tNKnrr1LOor3ACEojE+eu9ygpZiXrm7mxMnliXP+dzmVho6fMyrK8X2yk95OX4h51x6dXJ/Y5efLfv7+Nblc3izoYs/L9vNR0+bTFXRgOXEd74I+9fBGbcmN63co+9py34P1cU68WZzi4c7XtrJT66ch9vZv9uev+/vfDC+EXjXQX0u2WAsiBFmyZIl/eYq/OpXv+Kkk07i1FNPpbGxke3btw9qM3XqVObPnw/AwoUL2b179whJC/9Y3cw9rzagXvslrMiwAuXmf+ntzw3R+SsFS78Cr/0CvLkZ5fRj02Ow7SkIeiDUR3dXJ2Xxbi6xryC6+83szxMNwxP/Cctu1++b34bVfyH22i9Z29jDjjYvT6zrn1wXeuFHvKf7j4CitS+hIEKEo3HUstuhdRP5u1/gU3lPcFJRD85/f5cvRn5HpSNKcOk3Yfnd4G2FsBdW/I6p4a1cpN7gthPDPLW+5R1ZEQ+tbOLOl3ayobl30D5PMMIN967gte0dxOOKpzfuZ32G4w6Vv7y5hzPiKzit51+wbw0XzKmlwxtibVNP8piEO6bLCui+vK2d5bu7qC528ZVLZvH582cAUF/uZm+Xn2Akxh9e30WvP0KRKw9vMMrDq5qS7dN5cGUjd71suWx2vQKr/wwrfsfzm1upKXbxp08swRuK8sbOlKtpzd4ednX4+NGV86grK2DVgLjH3a80MLG8gEfeX8iXHA+j1tzfb3/CQjp/Ti1fuGAmgUiMG/+wAm8o2l+4FffAq79Ivu3xh5Puyc0tqcHsn9/cw7/W7uPhVYOXI5/kX8/M+I7BD/4wMKYsiOFG+iNFYWFh8vVLL73E888/z7Jly3C73Zx77rkZ5zK4XKlRh91uH1EXU2OXn7gCAl3Q29x/p1Lwxq8Agd2v6k60bkH/Y3a8AO2b9eum5TD78hwLvBwqpsNNLxCPKz70i5epr+jhjz03EPXsp9cfodSdxWzzDQ+Ddz84i/X7sA+ABe2PM6P43VRWV2jLIIGvA5dnF+MErpgUYbsniFKK9r4QdmJIxAdn3MqWjWu4uu0FXJtLQSlswW7uHPco8/Yv1+cJeUDFAXgidirX573A4qoI0bhibVMPp07rb11m/VisUfdvX2kYZIl89eF1bG/t4/cfW8yc8cUs+eELvL2nm4WTywFYbwVbf3/DIiaUFfRr+881zTy8qok/fXxJxlTraCzOfW/s4fsVPdAHeFs5d9ZC7DbhgeWNnDypHKUUz29uA7SCUErR4Q1R4LDz50/0nyM7qcLNmw2dbGvtI67gtotncdm88by4pY0b71tBQ4ePygGj9E5viPa+EB3eEFVe3XGrt+5mWd8sLpk/lZk1xTjtNhrafck2iVH8GdOrWDi5nOW7ulBKISKs2tPFqj3dfPeKuTjf+m8AHP52Wj1Blu3s5IdLN+MJRJhRU8TUKv17v+O6Bdz0p1V86cE1/PYji9KE2wGhPj7zl1UsnFKRPB5gc4te1E8pxQuWwrnn1V1cd8pk7LbUsy6PtOMrHjfo2R8OjAWRY4qLi+nry7x6Y29vL+Xl5bjdbrZs2cKbbx7ECHeEaOoO4CKMRPzgGaAg9r4Jzavg/P8CVwks+83gE7zxKygaB3YnNL6VW2GV0teo151Kwg3wwXN0h1hFL283ZpEBoxTqjV/r12GvPm9Yj+pcKsCHbC9QWuDoryAalydfnlXQQKsnSI8/QjgWpxBLobuKecT1PgolRN76B+DED0PdIubvfyR1nlCf/gO68icBMN2t26/a01/2eJbpnkopGrsCOPNsLF3fQmNakDccjfPCljauP3Uy5xxXTU1xPpMq3Kzc05U85v9e2MbmFg/3vLqr33mjsTj/8/RWXt3ewb7ezJM0V+zupsMbYmGhNTr3tVPmdvKx06fw95WN3PnSTtY29dLcE2B6dSHhWBxvKEqHN0R1sWvQ+SZWuGnxBFnXpC2cOeNLAJhWrTvWhnbvoDYJq2JLSx/4tCISXxsXxl7hwrk12G3C5Eo3O9MUxNt7upleXUh5oZOFk8vZ7wkm7/HuVxooLXDw4Vk22KA/uyrp5dmN+/n5c1spdNq5auFEvnNFqoTcu2bXcuv5M3lmY2vKMojHtAtTxfj3ht388vltvLytHbtNOGVqRfK4Dc0eWj0hLpxby94uP0+nxdK8oSg1dBAuHJ/x+b9TjILIMZWVlZxxxhmccMIJfPnLX+6375JLLiEajXLiiSfy7W9/m1NPPTU3Qjz2aXj9VwfdLByNs683QDmWggt5tOsmwRu/hoIKOOUWWHiD/rH8oBYeuE7vb98Gu16GU2+B8fP7daI5oXsX+NqhfgkAv7PcAJecWI8qKKfa1suq3UMoiGe+qWX/QS3x79cgbZvwlR4HKG09WJ323rwpvDf0BKX5eXiCKXdBaPcywspO2FbA7PBmuv2R5Ki9RCwFkV/Cc101bHZbI8jTP6f/gOaC4wAI9HUT8fcAUDlJxwsKI11Mry7k7T3d0NMI/zOd6Pdq2PL9hRk7xH7sfYv4/82nMtrCjWdMQdDB7wQb9/USjsY5ZWpFctuiyeWc2fB/qB/UEv/BeAq2PkaRK48HVuxNZVT17Sfwvwtw9WrXzeYhYnvPb27FabdRFbaq53h1B/3Ny+bw7hPH85Ont/C+219HBD5oxYc6vWHa+0JUFTl1m7Af7joT1vyNSRVulIIXNrfidtqZbMUMJpa7U1ZAqA9+OQ9+UIv65TyiAa1MNrd4wNuOKqljj3MGH8t7jtOnVwFawezq0M9SKUXDnt3cH/ocbHs2aUmt3N1FQ7uXZze18pFTJ1Ow9k/6+BkXUmvz8PPnttHYFeBrl87mh++fx1kTBH69CLY9A8ANp03B7bTzu0QgvmcvxLTyKiaAJxjlz2/u4fgJJSyYXM7Odi/haJznNrfyLvtq7uy+mfkVYf64bHfy+bb1+KilG1VSN/z34BAZUy6m0eJvf/tbxu0ul4unnnoq475EnKGqqooNG1IV02+77baDF2DnixA8eJ/yvp4ASkG5pHVCnmbIL4GO7aitS4me8SUcTjec+Z+Ql0+84WVk+3NIPAb7VgMQmXExsd428t/+vfbt5zkP/h4G0OULU+529HdrJBRQ/Sms2tPNyj3dfOeKueTZbVBYw7SYnyf3DKEgmlZCYQ2c8H7WN/XyzI4A0e4CvmHbpq0Hy4J4Mu98/iP4e6bQQm8gNYIP71rGdjWVyVXV1Ps2AO9P+vGLLAsiYNP+85VnfYM5E9th3AlQMwcu/SnN/lrqXv4ore1tON0hJgB1E+qgpRy8bSycXM6zm1qJt7Ri83ew3zmVueGdXPj7l/nLp8+ntiQ/83298j/Ye3bzSftS6qdcxps7O1nb2JPcnbBKEp0gwILJ5ZyycRXRsnpCnnYuy1vJ1R/5Itfd8xZ/Xb6HT587A9W6kWLvLi4r2slv+iawZb+HC+bW9ru0dh21ctb0UmxNVhaPNYK32YRffGg+Z86owhOMMLmyMBkU7/SF6fCGmFJpuVvW3g/718OLP6L+ivMBeH1HJ8fXlSSD3P2sgNZNuvOdfCay5zVOlu28ok5ic4sHFWmjOVLMc/6p3OB6CYfDDsC06iL+vaWNaCzO7k4/740spUbthRd/yOxP/Bu3085LW9t5eWs7DruNG06fAk9shupZSN1CSnc8T58/yJTKYi6ca7l7VtwDndvhxR/CzIsodTv48OJ6/rxsD7ddPIsJnam4wbwqoa+oguW7u1gwqZw540uIxBQ72ry8sGk/vyp4jLzunXyp7lU+vvsCgpEY+Q473a2NTJM4jvKDSL44CIwFMRYI9SU7uIMhMQIulzQXmRWHUMtuJ0Iet+3Ro3XcFfCub/FI7BwkFoLeRu1fFRu/Xh3lW6vcOrNo/7p3fDv/XNPMwh88x//716b+OeiNb2lXV/Vs7n5lJ6UFDj6UyFoqqqHO4WVtU0/mvPWwD2qPhwu/xz8qbuI++weI5ZfqfSGv/gNejp4AwIzwJsLROMFIDKJhCtrXsSp+HI4pp1LWtxU3QdZbbpBi9HNs9uvYR+2U4+EkK+PFZodTbqZm0iwAuro6aO/QnWhtdQ0U1YKvjUWTK+jxR+jcp908/1JnAWD3t/Nf/8y45IruKHc8T9hRwofsLzPZHWL2uBLdUVrPYNWebuorCqhJUzCLJpUwWfbzun0xL0bmcrprJ2fMqOKsmVX8edkelFLsbtQWwbsn+phU4U76y9PZ2e5lT6ef90yOQNyyttISFZx5Nq5eMombz57OxcePo7JQDxy6fGE6vGHtYorHdKJAfin07mVm57/1xxWLM3tcSb/rJa2ARMd78Q9RYmOBbTs20fNPetub2eot4LipU3DEAhDVyQTTqgqJxBSN3QHWNuzjI/bniLlKoWUNeU3LWDi5nEdXN/OP1c1cuaBOy+ZpgpI6KKpGUFTg4aazp+n4QCSgkw7yS6FlLex+DYCPnzEVBfzh9V0pOYFzJ7u4+expAJw6rYI543Ts646XdlC0fznTo9shv5RT2v+BPRZMJhv4OrTiLaialPk78A4xCuJIJJbm245HIR4/9HPFYxDuSwZZB+Ft18dkIDEhqZx0C6JJm+lr/sbD0TN5fX/qKxSOxnmyRY/6Avu3Qed2VNlk/rmhk5f9VuZWIg4Rj+kfTvOq7KybaBj8Xby8rZ0vPbiWykIn972xm9tf3KHdXs2rYPfrMHExu7oCSTdAocsykotqKIt34w/HBmeSAER84NSyN3YHmFxZyAlTtdkeDXq0gnUWsS40joC9mEk+PTHJE4jA/vXkxUNszptN8YzTERXnJNvOpJ+8yHIxtUe0T31goBdgfG0NAL09XXR3a/9/3bhaKKwGbzsLrBF+R/NOlN3JWz49Wr9qlouXt7X3z3AK9enn8fJPwOHmibk/xy0hJu+8nznji+n2R2jrC6GUYuWebhZOKu8ny0xnN06J8WRLEXvcJ1AaaYfeJj4wr4pAbzsbmj1s360VxHRbC3PGF/fLuAlGYvx7Syu/f00rs7PKe/QOV0nSgsDfBZH+cYsKS0G09QXp8oV1SujWp6BrJ1z+C6iYTvmau3DatdUwvzKWOkcswvFlEfZ2+Ym3bwObA2pPwF82iwWynXkTy9jZ7iXmaSWcX8mZJx6XkgNtQYCOYci6B6iUPuTKe8FdCa/8jF+eFedvlzm4/90uvnGxzqiitxlK67TlCfz5w5O5ZsF4bTm/8Wvwd8JV94K7Cl79GTSvor4gxGXzxnP/8kbCran5E6dOdHLB3Foe+8wZXDR3HFOrtEX1xLoWbit+BlVQCVfdizPczZX2V5NB9Ein/hxKxk0Z9J06HBgFcaQRCUDrBohYgcTOnYODwwdDwnIIZ5h9GgnCr06GVfdlbNrYFUAkgwWx/kFssRC/j11GhzecDNau2N3FhqDuuNp3b4DOHfiLp7Kn00875XQ5J6QUxIrfw2/Pht+9Cx7+xIHvY9lviPxqMZ/+ywpm1hbzwpfO5X3zJ/CzZ7fRd/+N+jwdW2HyadzzagMOm42Pnj451b6wBndEdwYZJyCFUwpib5ef+vICysq0X76rqwtCfShnEf6IorXkRGp7tSXUG4jAvrcB8NecjNQvBuAk2cm21j6KXHlJC6I9rDvATMFXl1tbK35PN95ePWmrpLQCimrA18a0qkLK3Q78HXsJu8fRrnSnfvq4GMHIgJnJ//yMfh6bHoOTP8Jbsdm8KSfhWP+3ZFB3U4uHpu4A7X0hFk6p6CeLvUuPbPsKp/DhD1ylNza+xWUN3+dB5/d4bnMr+1t0uqWju4E540vY1enDH44Sjsa56U8r+fh9K7l/eSMnTyqjImC5lyadmoxB8Lvz4OUf97tuZaF+Lttbvann9MavoWwSzH0fnP5ZpGUNl5U0UECQDyy7Us9pAFh2O7es+xC2WIjA/q1QMRXsebSXncTJth2cNb2MaCxGabyHyZOmYHNb9+zXz3qalT20tbWPmS3/Yo9zBraZ58OST0HDi1T+7RJO//cHOe35Kyle/Vv92/F3QMlE/RkBs4tC2N78Ddx9rnYrTVgA08+HUz4FDS/pz+T+a/nU2dPwhqI071xPAG25TS/Wg8D59WXYbEKe3cb8iWUsqoqxOLwcWfwJfa7xJ3G969WkazDeqz+Hwqq07/phxCiIIw0raEU0knofG5zbnQ2tniCrtlk/zkwupkC3ti6sWMHuDl+/iUeN3X4mVbipSCgIdxV4mvFue4Xd8VoK63SWxq4ObZ08t6mVLinBowoI7NsMnTvZEdf+2JPqy1gRnYFqXA5KEdrxEkH3eJi4BPpaDngvnn3bcAQ7mOP28sePL6a0wMF333M8bqedQMsWmHwGXPcInfNu4uFVTXxgQR01xWl++aJqHFEf+YTo8KaeZ28gomcyWwpCKUVTt5/6CjeV5TqltLOrC8JeYg7diXRXzqekbwcl+OgNRFDdewgqB+PqpkFBOcpZzHhbD9G4oq6sgMo8rZD2h7SCSIyU+2GzE5ACQr4e/H09xBFwFunRqbcNm004b1YN4mmmO6+GdqUVynFFAYpcebywJW1msqcFxp0I1z0MF3yXxm4/TQWzoaeR2bX6Hja3eFLxhwEWRML18bP/uJLq6QvB4Ya1f8e15TGm2tp44K09xK2RN927mVOTj1KwZX8ftz20lle3d/CdK+ay9PNn8cePL9Hnc1dC1XE6icDfBd27oaP/nJ8Cpx23086W/doamRHaBI1vwqmfAXsenHQNuCu5QT3OlfZXcQQ7tBUK0LIGZ8TDCbIL1bEDKvUov7FoHsUS4IKqbsrwkidxZk6bpl2ioNO3gfJCJ+VuB/e+tpvSWDeuCcfrCYtnfgGufwSufVD/FU/Q8ZDEoK20Tlt5oK2j/ev1Mdc+CNc8oM9xhnWO6edD9y5OqCvl9OmVuHobWK+0ZW0LD3bR/f5ji/jLVVY8o26BPteEBdRLG2/v6UYpRZ5vHwHykYKywd+pw4BREEcaCXePiqXexzO4RLLgzpd28t2HrNTZSAYLImS5BawO4YdLN/OZv76d3N3YpRVEbZ6foM2tR2W9TdC0nNUclyx90NDuTQYkz51Vyx6ZQHnbWxDx82ZvBSfUlXDdKZN4NTQd6WvBs78B7443WNo3nQ7XxP6ZUUOwx/J5335xUbLjL3M7+dDCiZSEWvFWngAzL+DFBi+haJyPnDZgRGW5Aaqkt19dnB8/tZlP/XklylIQ7d4QwUicSRVuqqu0gujp6YKQl2ie7lz9tXrG+Mm2HfQGIvg79tCiKphtjc6lqJo6h1bINSUuqhzaDbIv6KCi0InDnvlnF8krIh7oJeLrIWxzg80GRdVWkNzPBXNrqVEdbPAWEc3XnXqev6PfzGRAH182CWZeCE43jd1+4sUTQMUojXZSV1bAlpY+nlzfQrnbwSzL352kczvkl1FUVqs75rqFsP0ZQOEgQtDblUpcUDHmFWhFc9tDa3l87T6+csksbjxjKnMnlFCS79Dfr8oZuiON+FOdum/wxMmKQidb9+vOclbDfdqHf7JVNcBRAEtu5uTgW3zJ9ZjellAyHfo7vMS2lXzvnqSC2O7U39Hjo1s4oVR/7o7ScVphQdLFBNrN1OENUW7zU1tjBdzzXDDjAjjuYv1XO1dfM6EgSup0nAi0ddS5Q8eyjrsYihPncOpzjD9R33M8zq1n1TFBuqiZfbo+JjT4N1Cc7yDf35K6DkBpHUWxXrw+L7s7/bgD++m0V2vlkQOMgjjSsCZJoWLWazVkjOBA7O704YwmXEwZYhBW6mZCQWza56GlN5j00Td26ZF0td2Hz16iv6TNqyiKdNFTeTIn1ZdhtwkN7T62tvbR1B3gwrm1dBdMpiakLZeXOku5YE4t75pdw9tqJgD/+OMvqVTdbLTNZl17POOPI51oLE7Eq10oteH+M0lvWlRKvkR4s0P79Xe2e8mzCbNqB3R61o+4ml5k7zJ4/Vf0vXw7T7+9g3zCCAoc7mSdn/qKAqqtkiceTzeEvUTsOqUyNm6BFfzchicYIdzVSIuqTLpvKKyh1q7vqbYkn4q8EDFstPhIpW5mQDmLKVB+HFEfUUdRP7nxtXHW9HJq6WZroISZ4yt0J+dr48I5tbT3pc1MtuIliWe3rydIXiLLxdPMnPHFvL6jg+c36zhN+qQrQHeAVTNTnY41ryTRqVZLLxNdfrBrl9C4aBNnuBrI69jCJ86cyn+cUqVn2Kefr3Jm0hXDnjf0/4S7yduuXTBAZaGTc4Iv8mn7Y5TteQYWfQJcRalzLf4k5OVTHu/WLhzvfj3AsL7D73W8SV48nFQQu6LVdFCKY98K/vThKcnPh4L+LibQbiYhThF+JL8s84dUOUO7fS3XDqUTtXwOt6UgdiavPYjCGj3YC/ZwSqlWqlPmnaH3hfr0fJuNj/WPQSYmp5ZaJV1K9P/x0snK3V2UhNvoc/XPHjucGAWRYw663HdCQcRj/PJ//xd/IJCyJg6SvV1+ihM5+BH/4GB3Ijjsa6e3u4PmHn3srnYffcEI3f6IdjHZfHikWH9JLVdV5eyzcNhtTKpw09Dh5QVrJuz5s2uIlk1PXqIhPp5LTxhPVZGLuSeego983h3Uy3tMnn8um7pAJX4cQ7BqTzfF8f7KLEGdTY8AX2vTnVVDu5dJlW6d2ppOkXYDVNt6OXPtV+C5b1P84jc4L/4WbiyLwlmUnEQ2qcJNXoHu8P19vRD2ErIURGFJGbGq2ZwoDfT6I+R599FCJTNqipLXqkI/29oSF+X2IH4ppN0bHlyLJw1bQSnFBCgWvw7oQtLywdtOcbQbh8RSyshyP50+Q3fcq61qp4S8yU61pTdILK5wV1tZLr1NzBlfQqcvjMNu46OnTxksyMBO7riLwVUK53wVgMun5THVHYIJ87Xc+9dyT96P+WvJb/jmpbOQf/8A/n69dnWF/dr1UjFlsIJIWBBv3g5/uRJiEWa6uvml8w6+4ngQXMWw5Ob+shVWaSVRewKc8Xm9bdcrEA2A3clsduttVXow0uWPsDlvDjS+hSQyqIpqBrmYQJfFOH9qgR4sDOWyqZyhExqaV+n3JRMsuaq1ZRTxQdUQCiJx/942PWcHtOJ0FGolt281PHSDDswn8DRDXgEUWG7AUm1JHF/o5bE1zVTF2gm6czOLGoyCyDmHrCBUjF/+6lf4A0FtQRzk4jLxuKKpO5AMkAKD3UyhlN+zcXsq/bShw5saSZe7KaePHkqSZm6fKmDBotMAPepqaPfx3KZWTqovo6YkH9c4nSHiU/kcN2Nm0oXxs6sXUTj1FKroAWcRl77rfPziRlQsswvM4vnNrak4SOeAWlXWCGuNp5BgJEZDu49pVUUMwupoT3XupjjcTvSsrwCwZLyNSqc1YnMWJhXExHJ3Mmgd8vVCyEtQtJVSWuDAVj2LqbIfjz+EO9RBl706lTFVWENpXI8Qa0vyKbEF8Yk7lbo5BM7CUoolQBEB8tyWgihK829bbo19qpI544v1Pl871UUu3E57Mi053YJIZqKN0ymUeJqT6aFXLpg4WGGFvNC3r7+CqF8CX9sDU3Rq7X+eVkqF+KBimu4Yl91BQcxLdagR27oHYPVfdDvv/lTWUtG4lLJrXpmSM+zXcxbiUfB1MNmpLa/Pxb+EfHknlGSYIXzxD+E/XocqnRrMNqtDTSvjEi/Xg5QOb4jdBSfoDrnNWlamsFq7jpxF4E/F3C45YRz3fMi670SK80ASz6XhJW1ROayMtKKa1H0NaUGkfZbpFkh+ibaiE7G4nrQS8L1NWikkrDnrN3jlDFi+o5VKeokXTch8vcOAURA5Jr3c95e//GV++tOfsnjxYk488US+8x1d4M7n83H55Zdz0kknccIp5/L3fz7Dr+68h337Wjjvg5/ivKtuOmgF0e7VReISKZbAYDdTmoLo2pNak2lnu49NVtrizNoiSuijM16YHL1sc8xiYqXu9KdVF7Kz3cuaxh4unGP5+SfruQINahw3n5OyJoCUu2LiIqpL3VRXWT+aIeIQSime27ifsoTPe4AFkeg0m+OVbG7xsKfTz/TqQgZh/TgvED2Zbkf5mQAsrLUzs8z68Vn++upiF/kOO9jshCWfSEDPI/FbGSclBXnYqmZSL22IpxE7MQLutI6sqAZ3zIODKDXFLkrw06cKdC2gYSwIZ2EZpRKgRAI4C8ssudNGnVanMmPGLM6bXWNZEK2ICPXllnssaiU1WBbE42v2ke+wMWvKRD1S7W3mzBlVXDZvHJ85b/pgIRLP1xqBJxFJGwG365F3QYUeAYf7dLJB6ST41616NJ84Ln3UnmgfTUtv9bWl3Ci+Nsbn6e9kwF134AmVFdMASc5UZr6OVXiUm6awvv8uX5iWkhP1/i1P6vTXxGi8oKKfiwlIWdVDuZgSz6VzRyouAPqzSNxX5czB7aB/rKK3WbulCsq1pRTy6EKN0D9r0dPc/zrW69Org0xzebCJwlaWVlH4MDO2ZlI/9TWdZXA4GTcPLv3xkLvTy30/++yzPPzwwyxfvhylFO95z3t45ZVXaG9vZ8KECTz55JPQ20RvSwOltZP4xR338uJDv6WqohxUFDjwDOTbHlrL+bNrqLJGqv0tiIEKItUpR9u2UVE4m0KXnYZ2L+19QUry85hRXUQo5qFDFeF1jaMICI1LFRubVl1EJKaVV2Im7aQZWkF0uiZxzoyq/tdMKAjrv72gBLqxlFX/0eLn71/NmsYeero6sOfH9aiup1GnAidGbp5mlM1BB6U8v7mVcCyerMvTjzwnFJQzKdBEQArYFJtMvXJR7QgxvUzAAziL2GsF5pPPJa8Qe8CHCnnxkbIgqJqJXRQ1nSsA+o/irI6wkl6qi/MpJEBHLB9/ODasBSGuYkptQfIAW37CxZQYdbYnraxvXH0BuPP1dawOuL6igKZufypbzVlMmyfIo6ub+fDiesoKXVrBe5opdTu447qF0LgC1jwP531d+72f/E9otQYKmUbBBRUgdj0JMuwFdzlUToe9b+hsn+7d8Mw3oHqOLtDobYV4JHUf7ipAAAXVs6F9i+4sEx2it40a6e33DIfFkQ9l9XrE7SyCaecScxTREKqhtbWPSVWFdPnCeCcfD/ud0LFNd7CJ0bi7vJ+LCYBAj/4/lAVRPEF37BF/Ki6QLq/DnXI7DSTdxZSYZCei3YmhvpQyTVgXoBXJ9PP637O7inx/C9fNroet4KrMzSxqyLEFISKXiMhWEdkhIl/LsL9cRB4VkXUislxETkjbt1tE1ovIGhFZObDt0cizzz7Ls88+y8knn8yCBQvYsmUL27dvZ968eTz//PN89atf5dXXl1FaUmwFptOshiwC1UopHl7VxN9XNiZdJVlZEGWTcfY2MHtcMdOqitjV4WPVnm4WTC7HpqIUxL20RgvZGJvIU7HF5M3/UPIUifzxurKCZGC4oKiETfXXMPHcjw2u8Dn5NJj9bjhB59fbCxKzlftbENFYnH+t20ehK4+PnGi5jCYu0c+kK21Rmd5mKBmP2+ngyXXaRE9MehqENRrfIDPZ2RXESwEltiCTrXh2yJZPY1eA+vLURLa4s5AS8SPRAH0qn3yHDVeeXXeMwJRerSDy0ksdWNf5zOISTpxYSqHy4VH6nMNZELhKKLUHqHGF9agSkooNb2v/USfoDifig7CP+gpdxkMlnqOriPve2E0kHtfrIIDukBKdsVKw9DY9FyHs153n23/SI+qZF+uU1IHYbDoG0LFNv3dXwglXwknXwnGXwoKPwqzL4d3/q/f72lKB6KIanRGV8P1P0i5K+lrAY5VN97ZRSQ8AzpIsFASkRuuV08GeR+z0z/NA7Hw2t3iIxxXd/jClxcW6FhikFC5YFsQABZGwIIaKQdhsulow9B/ZJzr/yulDZxTll4EtL2U1WRY5rmJtQSfccYnPKBbVbrqBdZZK66C3mSsn6s96+uxDXyfkQOTMghARO3A7cCHQBKwQkceVUpvSDvsGsEYp9X4RmW0df37a/vOUUoe2wnkmhhnpjwRKKb7+9a/zqU99atC+VatWsXTpUr7+vZ9w0VmL+a+vfrH/AVkoiMSyiG/v6eYka3GTqry0SWEDFUTQA84iVPVsqrZtZs74EuJKsayhk3A0zhUnTtBzJYCOeCGb2sP8v8gXeXXavOQppluB2Qvm1PRTBnM/cVdmIZ2FcPVfk2/zrAlisUAv9rTDOrxhlILrT53EdXXtsA1tdex4LpVKCOBpRkomMstZnFySclpVBgsC9I+4YytvRWfQ0O4jaCvCFvJQb01S2tkDLb0B6itSP0hbfjHVffq8nphLp21CsmOa5ddpwfnppQ6szuIj89xgt5Ef99OHDiQPl8VEfgl5UT95KpYKUkMyGI1I/xFwmvupvtyNPxyjt6ebMiDmKOKvb+3l4rnjmJJ4HqV1sN0qvb7ndWhZYz3DfSk3z/vvhkn9S2wPeoZt1gp5BRV6dJsY4bqK4Rqr7pizqP8s/UTHXFijldDkM2DVH2D/hlQShq+NsngPXaqIipKUFTcsVTNh5wvJz8N53ldZvuolZrZ46AlEiCtr3kn9El1uPt0ycVdqqyedYI/+P5QFAToI3bo+1cGn399Q7iWwFKyeGY+nWc+LAB2D8OxLKdPEZ9HXomOSpQMURMlE6N5FYdvbUDQOV9WUoa/5DsmlBbEE2KGUalBKhYEHgPcOOGYu8AKAUmoLMEVEcpezNQqkl/u++OKLuffee/F6tRugubmZtrY29u3bh9vt5vrrr+e2z3ySt9dvgXic4qIi+rzaEogfhILwBKO8tLWNcSX51DjTJtmFfSildP0g0KN2VzE97slMZj+zawuZVl1EOKo7zIWTy5MjrB5VxOq9PbjybNSllYqoKnLxq2tO5rPvGuaHMQwuy9futzrhBK0e7c+tLc5PjfKsWcr9JlhZQbzEHITSAkfmiWiQ/BGviM5kXVMvMWcRhPqY4Nb3e+cbLcQVXDAn9RV0FJRQi1aSPTGXdi8B5JfQY6+gTPXgVflUVVYPuk5iROiKefFaFsRwLqak1RAL9VcQRTXaxZQ+6kxsB/C1J1dCa+vUPnWvctEbiHDa9LT1I0omakskGtYzlBN4mvQfDO6MBlJYA71WEDVhDWTCmgGOt02PnPOs+04E3SdbFoQ1SVML3U5htIsOVTq8pZVOwhWW5hKbPb6YLfv76PLpwVFlkSvl2ixMVxAVg11MB4pBpF+rJIOLaagAdYLCap0E0Le/vwUR8qSyuhKfUXKuxYAYg2VB6NL2S3I2BwJyqyDqgMa0903WtnTWAh8AEJElwGQg8TQU8KyIrBKRAbluKUTkZhFZKSIr29tHYMWygyS93Pdzzz3Htddey2mnnca8efO46qqr6OvrY/369SxZsoT58+fzw1/czrdu/SSoGDd/7Douvf5znHfVzTR39dEXjAx7rVhaIHttUy/1FQWU2YN6Vi5A2Mdja5o55b9fwBeKWgqihGbbBAokzLxiH9OrCvmD4ydcn/c8J9WXJYN4XRSzurGbqVWFyQqaCd5z0oThO75hyC8uAyDYl1Zl9c07Gff0TYDSVUoTgcTSeu0Dfvkn8D/TdJVazz4oqeu3LkCmhWuAZJBwdXy6Tul16eyRGpdWmMubQ5wytULft0VeQQm1omXrjqYpCKDTpd1KLaqS8en1ldJ9zYAj6qUP3YFXH8DFlHowAxRE41s6tTK9s0goIm9bMm7S1aVl7Y3p6/RTlqV1gNIl2Lc9rWcmg+5septBbDrbaDgGjsCHImH1eFv7tymq1fdZUqddZUkFIeBrwx3WCiJTvaqMJDrktKD6nHEl7On0JzO4KhMWxED5Cyq0QoilTUQN9OjnkFDWGa9pXaufsq4dJEdGimqgZR2gUq4jV6kVg2gjGaPpa0nLdBpoQdRBqBd69qQUX47IZZA60690YCrOj4H/E5E1wHpgNZD4tM5QSu0TkRrgORHZopR6ZdAJlbobuBtg0aJFB5fqM0IMLPd966239ns/ffp0Lr74Yv2mfas1ZyHG5z71MT533WUANKs4/nCM4nwHQ5GwIBKL2dRXuCntC9BFqU4tjfhZ29hLbyDClv0eFob6wFXMvryJnADURhspr6ii1r6Wk2U3hbYfJUdYPaqYxq4Al807vDnXhcV6FBq01kAAYNcr1DY/yylyCrUlF0CjNcpzV8ClP9F59Bv/AU9/XQdBSycy14p/ZExxTbDkJrbaZ+L5tz4mz10Kwb244jpO4yefT50zrV8TcRVRINoK64w4KS1NPf8e9xTwr6VFVTA1vUNzFuqMIV87RILY4hH6VAEiQ5TZSJDeKaW/Pu2zusMVgfnXpbYnFVErE6fp63f36GfVFXUCgWSFVCDVIT37bcjLh3d9W5fS9jTrv+LxOk4wHAN9+ENRVK3XA4lH+4/az/iCrqskord3WAXrqmaCtxVnoJ1ZM+axcH6WqZtTzoKLfgizLktuSgwW/rlGxzYqCp1QXAXvu1O7thIk50J0pyybYK92Lw03Kp/zbvD/d//OeeJiuPi/dXxtOAprdA0n6G9BhL3Qp/Rz6NiW+kwgQwwibZCQYwWRSwuiCUgPr08E9qUfoJTyKKVuVErNBz4KVAO7rH37rP9twKNol9WxT8Ifq2J6Yptoz7ydOKHo8FVdo3FFRaGTJdbiL/XlbooIsN8q7EbYmwxeb2rp0zGI/BL22vQXrsS7m5qwdh+UqV5Y90DSvdOtdKc6bAd8CBSWlGnZfWkVXS2L4ea8J7R7wN+ln4OrFOa+R8eSTv98ainTkjpmjSvB7bRzQl0JQ1I5HXXiB5Nv84vK9MjNyg6qr63i3OMGBEedqfttDtgpSbMgfEVTAGihcvB6DEU6BTURfO/DTYXbOXgCXzrpVkO6NVG3QN/zJT/Sa0gkSMtwKnTlUVnoxOvRFkRnRCuGfstvJjqW9s0w/9pUHaHeJv2XzaIz/SyIYRREYZqLqShNqYw7QXew6edyFOqguLcdfO1U1k7UiQDZYM+D0z8LzlTM4oS6UmyiFYTdJoxLfDbzr4XytBIsCQso3c0U7BnevQR6AHDaZ3Sp9gQ2u97mPEDsJP35JazBxOce8enZ4ZCy6lwl/b8XkPqc7C5dviOH5FJBrABmishUEXECVwOPpx8gImXWPoBPAq8opTwiUigixdYxhcBFwBBF748x0mY7R6NhsNlR2LATIxQZHIeIK5V0PcXiivGl+cnFX+or3LiVj/3xMn1w2JecTLWlxaM7R1cxTeFifCofe9dOpFOvEKYKa+CN3+hURKAbS0FkSiF9B5QV5uNV+clVvwDwdxFHON++GnvndivnvlwH+RIs+GiqEy2t0wXrvnQO1586fFXLdN92UUmF7sDDXpTdxd8+dcYg91m6gtjnt/dzMYWsGeO9jtrkYjepk1suFitTzKsKDuxXT1cKw7k4EtgdehRvubLqK9z4vfo5doS1nBWZLAhEF8BLbEuMVg8Uf4CUK8VZlIorZDyuRo/M+1pSbTIdA2mKykqfTbdSDoFxpfk8+8WzefBTp/HMF86mfCirLZEN5u8CnzWqD/QMH6B+p6QriHQLIoE1O13HhZozK+1EuwknD/8ZHAZypiCUUlHgs8AzwGbgQaXURhG5RURusQ6bA2wUkS3ApUDC91ILvCYia4HlwJNKqaffgSyH2nTkUXES3rlIKIQSO3GxJy2IgffS44+wq8OHP6QXth9fWsDZM6tx2m2cOLEUV8xHmyrTpw77kjOkN7d4kjGInkCERnudzg7q3A5iQy78f/r1m3cQd5UQQH8Rh0whPUTK3E68FKACaWmugS5WFpxJGIcuRe7vHDxazS+BRTdqy6JUG6rjSwuGLISXoNztxG4TivPzKCgu0x1SqA9xuilzZ+hI0uoA+cjvZ0HEqmYTV0JfYYbFWgr1LOdE0LMPN1XFB5jHMlQMYjiKxyXTROsr3HrWN9Aa1HKWu9Nckq4i3VnPvjxVDqJ0orYerFjOAUl03sO5lyDVEUb8Q3f4CddTSZ0+PjGHI5s5EAdgRk0xS6ZWpMqfZCLxndryBPx0hl5PJNg7dIrr4SBxz67SlGJI/9wrpul9vU16TkpZhjkOxeN1uvOUMwbvO8zkdKKcUmopsHTAtrvSXi8DBkV1lFINwEmHQ4b8/Hw6OzuprKwcOnh5JKHiemQYC5NHFGwFxONx7MSJK0U0rnDYU/cRiMRQSrG/rYPd3REmlLmZO6GETd+7mDy7jVjUi4dCYvYCQl4PgUiMfIeNLfv7UPl9iKuErs4IrY56Zndu11/assk6gFlYAxE/sdLJ8GsdMDvcFkRJfh67VYGejQvaggp0s8M5geL845mz9w09Ws0UED3vW3o+xXCujgHYbUJFoZMJZQVI4ofpbetnKfQjbbtPFVCSn/rJ5FVM4d3hHzK1ZvHgdkU1OlZiWRABm3v4ADUMiEFkqSDGnwQ7XgClmFRRQHxTH8rhoC2gKHM7Bru0PvZk/w67pE7X/lGx/r7toUh03u7y4Y9LjzsM1eEnXE+ldf2PKXznCiIrEt+p5XcDSgfMgz2Zy3scLtLvOUH6515Yrfdt+Ie2nM/7xuBz2B1w04uZlcdh5pifST1x4kSampo4EjOcBqEU9LZq32JMp+jF8wqIx2PE4tCufMS7nLgcKd9ne1+IYDRGQ9TG/y7r5NPv0l/6PLsNomHssRB9yk3UXoDPp0fpZ8+s5vlNLYjNq9M1/WG68idBz8v62olKnjMvAMABFDpbKHDmpeYBHCby7DZ8tkIKExO8gj2g4uwLFzC+eh5zWv6mS1fXzM3Q2HlIPthzj6tmcqU7NUrva0nWXRpEmgXhJb+fi6nU7WCTmsKpZUPUfgp0Jf3bx02awKxpw2T9wNAxiOGoX6IDzd27qS93E1IB4s4iunzh/gHqBAOzbErrUnGvrCyIhII4wL1k0+EnLYiJAxTKO3MxZU3CCkqst9K53QpSl+XumulWU4J0l1ZRjd7Xtkn/P/79mc9TMzt3MqZxzCsIh8PB1KlTD/+JoyGdZjnrkkM+RXtfiHtea+BLF87SPuxAN/zkNNScKxCrXHLX9Pezr7UNV18TN4V+xA/ed0LSz66U4kP/71n6glHybGK5mPJ12t7WpcnZql4KCNsLCHo9FOPn+prdvLnJinW4iunyhekrnwK9Sv9IZl44SNbSAgcTKw4QgDtEQjY3pYmy5NbEvKaQm5lVx0Hbn3WhtcNoTv/0g5ZxutEKa3latKsmE049uotjI0D/NNfE6wll+YPbJTo5a9b3d646FSoOsG6ww61dZiqWXQwCUlksjcuprzifFgkRsbvp9IaTK7QNS3pHlU0Mwl2h00AP5GJKt1KG6vDTYxCjYUE4C8Hu1PdTNklXsR2pGMSBLAiAU/9DWwujiCnWd6is/gvc/+FBq2IdDC9uaeO3LzewprFHb7CWBQ24Uj8on7jpVW4q7X7yHbbk6m0A+3qD9AWjzKwpImqluE4oK4DVf4IHPwLrHwLALwUEpIBwoI9r7C9w1pufZKrNKgzmKqHHHyFYmpbemWGyz1ULJ3LVwtwUBQvnFeFIKIjExDyKiE1YmDroQCPWQyHpYmo9sAXhLOSc42qYnzZHYlKFmwvm1HLurAwdYLk1KNn+vHWeLDodEd1Z2PJStaYORPVsfR+NbzGpwk0hAUJSQJcvPHxKbYJ0t9LACVmZsNl12Yr0bKpMZNPhV8/WSnH8/P4K5R0GqbNGRM/IX/xJnabaukFb7rmMQRRUaGVUl/bdTnwPExMKJyzQcYYFH82dHFlyzFsQOWOvtVKbZ9+BJ8cMgcfKPtqy38OSqRX88qnVfAHoslWSGKt7KaA77qZI+ZhaVURDe2rp0M37tFvm+lMn853HdZG18SVO+Nft+oDtz+r/+SX4lZNY0MdMZxeiFO8qboQQRBxFeENR4uVpCiLD/fznRbMO6R6zIeYowhWwFJ+V4tqtiiivGqfTHzu2HXjEeigkfpgqNrSCsGIQNlexXj4zjXyHnXtuWJSpFUw7VyuJvW9Y18rSInCV6I4r23iZzQ4TF0HjcsaX5rNbgvjIp9MXZvHULBREwoKwObLvmG9+8cDHOAv1swt7h45BlE+Gb1olrhN1wQrKD1zF9XDyyX/rZ/3aL5LWa05dTDYbfGFAwdDEdyPxnBbeoFfRs2WZ6ptDjAVxqDS+pf9nWDYxW3oDWkFsbvHQ6gnywlq9iEirlXUE0BN30xkrwK38TK/KpyHNgthsleR+3/w63E47IjBu/0s6GykvX9fbASbW1tISsBMP9THFoX8Ei527AfBaqqiopEzPUobh68nkgJijGHfcui/LZ99FMTUlrtQM2IMIRGdNus//QBaE6yCztxJ58aA/i2w7vfyS7OMPCepPgbaN5EW8lNtD9Mbz6fYPEYMYSPF47WIpmdA/jfhwUFit3TXZpGI6rVXZRsq9lMBm0woi/TufSxdTJpyF+jNIv/cjQDmAURAHx8bHYP3Duo5Kj15SM1lgC/Qo6Omvp0YiByChIDa19LFqT3dyZbO94dQXtCfmoj2i3Q1zy2FPp5+vfPfb3Pvbn7GuuZdJFW5K3Q5uqN7OPfm/Ju/Zr+u0z4UfSwbfLl54HH0xJyrsZ7zoEfqMiHaN9ca1/7zc7dSVKJ1FQ/vjc4WrGDfWwkjJ2k/FevLZgOUuD/d1kwxpQVjHDJXlNBzzr03V+z8YmQ5aQSzR2W/NKym1h2gPOVCK7BSEPU+X18gmg+lgKarJvsMX0QrlMKS4HhLpbtVcupgykXAtjlRw/iAwLqaD4c07dG7yJWlVYX1pCmLPMn2Mswje9c0Dns5jKYht+/tYvqsLt+gCdW+2RLmYfNwE6Yy4aI24IA/eP6eQQKSSz675Hfubi/leeA4XH68nIX0275+47JvAMQnOvk0Ht97SGcWzJ9fRXliC299IZawHgOqgtlYa+vRIpbzQocs41C3MafGvTIg1YosHPdj8ncTETsCmZx5z3CV6wfe6IVw574T0jttxmC0I0Ernwu/p8inZcsKVw66ul5EJVrnn1o0USZC2kDVJLtuCdwtvyM2gYN4H+y1KdUBO/oguJz4aJBYfQuXWxTQUJ39EuwqPMIyCOBhiYe1Tff47Oh3UVZxa5ANSU/ZX/A7O/OIBp90nLIhAJMYT6/ZxUUEMYrCqJUywwI07HqTRn0d3XHde410hvlS5DFSAybYA1fRw/ITjIBqmsGMdLLlJL8cIOjMngauEqRNqKdu5jAJr1SuxymKta9MpjuVuJ0y/5p0+oUPCbi2v6evroTjQhc9WSnVxvp7VXFQD1z+Smws7i0h2CkNaEIVpxx4CBxtoXHLTwV+joFwvxtOxnQIVwKu0VZiVBQFw7qClWg4PB3sv53w5N3JkgyNfB4979oyOgkj8bo8wjIvpYIhZ1VT9nXrUVlqXWiYwsR20i2nNXwe3H0BvIJKc6drhDXPGJO1KCigXkTzdIW3vteFRlqLxtsObdyVjBb8/P85HT5sM+9fp7Iv6tCBqyXj9hQdwFVNXU0mRZaGkZ6us2q/TXbPKeMkReZZJ7+vtAn8Xfbbi7Ms9vxMSq3nB0Mrc8Q4VxEhROQM6d+CM+fFZS6OO5md6VJJwM410DOIIxiiIgyEaSmV61C9JFSRLkCgqV7dQu5oOsIaDJxhl4eRyPp33OP+d9ztmWGsj+8gnbvm+W4IOeqw6SDz0Mb3C1OU/B7uLE9VWXR4iETCfOKCeYf0pOs/bkY+ku0imnatvR9lYtV8rjbL0kgwjjKuoDACfpxv8XfSoopHr3BKB6qEUgM2m4xAHE0cYDapmQOtGbCqKz1p7onK4xYkMg0lk7xkFkcS4mA6GWBimvwvGnQhzrtDrEiTW8IVUUbnTPw8P3aAnq825YsjT9QYiTCoI8fm8R4kpkCI9sS2AC8kvAa+u4bNPVdJ84mepc/p1tsmsS3VRr8bl+kSNb2lrYWCJgLNu0/KCzhBJMO0cWPMXfOLGH4pT6LRnXz0zBxRYa0IEvN0Q6KIjXjxynVui43cM4w684peZZ3IfSVTOSK6GlrAgyjPVljIMzZKb9byIkUyzPcIxCuJgiIX1iPz0z+r3iYJsSml3RaKo3JwrdD2jN359QAVxtuc58glpV7hnN0rsRMSBu7gcOqBPFRDHhu+Mr0Ft2ii2fgm89Vtt1TQuhylnDr5AzezUlPx0H/vUswE9gxnIXKRuBHEX67o+IV8P+Ltoi47L3n/+Tkm6mIapMTXvqpGR5Z2QlqYZtrspdToOWLjQMIDK6cm1xg0a8w0aikhQL/uXTkJBJCiq0QvWJNJa/V06HTORA9/4Fux9a/B5O7YT2r+FSbG9LGl9COW2Mjda1iLOQtZ+52JKynTevxftLigrGOACqj9Fy7P6z7qW0IEWDkm4UAqrdcZKQQURh1Y4o+2rLirR9xrx9aL8nXTEC6nIpkzE4cD1DtJYjyTSJjcWFJWOnII1HNMYBTEUf3ovPPut/tui4f6TfhI53onJcoHu1Izf+ddpX+bbf+x/jodvhN8swnXXKTzv+gruSCdy4ff0vvYt4CzUq8YV1RJ2lhFDu35KMikIBJ78kn4/6dTh7ycRhE3MnK09nki+VkyjGX8AKCnXcuR3b0XiEbpU8ch1cPkHCFIfLZRP0ZOtgKl1tSyacoBqqwZDFhgX01D0NqaqPCaIhfsXz0pb8pHqWdqCSCz44SrSNVXaNvU/R9smmHQaLcddx4+WbuGac0/itPkfhKW36fz3hKvj9M+zo+oSuL+dAoedfMeAGEFRjS7d3NeiJ/aMmzf8/STOm5gQ9f672LBuHzzZPuq+amdBEa/KQk7d/xig6zCNmFWTtCAObxnzESfPpd2a3bv4yNknQP1hqZZvGOMYC2IooiFdsiKxQI9SloJIsyDSF6dXSscg0msGVc6AjrRzxOO6dtPExTRPvJzH46cTnnKeNdXf8n0mgqUFZRROPB4YZoQ/5QztH59xwYHvJ5GumbAgSidSOUFfc7RdTABPFV+FQ2mF3K1GMkhtWRBDTZQ7mkikaR7t7jLDEYNREEMRC+sV1xLuo3gUUP1jEOkupohfz0VIKwnxWk+5XggnUY7D36HPWzoxWagvWT46EWRM+3En1hMuHeheOhSSFkSqzHB9hRXfGGUXE0Bn1WK22nQHp11MIxWDyCJIfbSQiEMcyqxvgyEDOVUQInKJiGwVkR0iMmi6poiUi8ijIrJORJaLyAnZts05UV0XKVnOO+FuSk+BKyjX8x68bckaQomicvG44r6tlgev0zpHr16VjZK65Czq5ApliR93mi+80GnHmWc7PB14SZ22ThJlGYAJpQV87PQpXDh3iDWDR5AJ5W5+E30fUZuLRlVNxUhZENWzrJnIOSgGONJMOk3HvXJRt8owJslZDEJE7MDtwIVAE7BCRB5XSqU75b8BrFFKvV9EZlvHn59l29yhVHJFNzp3aFdOQmGkWxA2m5Xq2paaRW25mPZ2+dkSqQVX4hxnJtcOprSO3q6BFkTCPZAayYoIVYVOygoOQ2dZWAlfb+pXJdJmE777nuPf+bkPAxNKC/hDeAGlJz5F75o2Cp0jNC9jzhUw+92Hv5LpaDDnCr3e9BFSCdRw9JPLIPUSYIe1vjQi8gDwXiC9k58L/AhAKbVFRKaISC0wLYu2uSM9OJ0Y/SfKbNgHdNZF1boERqIOkzV629zioVlVEsKBK2GFeJr1/5KJeKxJTSUDFcQAX/g3L5/LuNLD5G45gjuO8daqbGtbAlQWOkdu/fCDWXvhSEdEW7QGw2Eil8OmOqAx7X2TtS2dtcAHAERkCTAZmJhlW6x2N4vIShFZedjWnU5YC6CDzJCyKAYqiMIancU0wMW0ucWDwsZuNQ6V7mKyu6Cwit5ABLfTnprMlMGCALj8xPEsnHwMuD8OwPhSHQ/Zur/PlIgwGI4QcqkgMg3L1ID3PwbKRWQN8DlgNRDNsq3eqNTdSqlFSqlF1dWHqZ56PwsioSCGsCDKp+i1bBPBbMvFtKlFlzneGR9PPKFkPM26VIYIvYFI/+BzfomeOzH9vMNzD0cZiXWdw7H4yE2SMxgMw5JLF1MTUJ/2fiKwL/0ApZQHuBFAtE9hl/XnPlDbnJKwINxV0L1LK4dMQWrQJS9W/j65ehsFeoLSlv0enHYbu9Q4bD1v63P0NifnIXgGKgiA992Rqzs64qkpzsduE2JxZWYBGwxHCLm0IFYAM0Vkqog4gauBx9MPEJEyax/AJ4FXLKVxwLY5xVozgdrjdXprz97MQWpIldje+aLOILHn4QlGaOoOsGhKOQ3xCUg8Ct17LAtCe8p6AxFK8kc/vfRIwW4TxpWYMtUGw5FEzhSEUioKfBZ4BtgMPKiU2igit4jILdZhc4CNIrIFuBS4dbi2uZJ1EAlrodbKuu3YnuZiGuD+KJ+q4xBhb9K9tMVyL501s5oGZVVYbd2gs5hK0xTE4ZjfcAwxvtRa6MbEIAyGI4KcltpQSi0Flg7Ydlfa62XAzIHthmo7YiSshcSkskB3avKRfUCnLqKtiC1PoNyVPLSykdd3dABw1swqfvn0ZALOCgpe/TmoWNKC8AQilE4wCiKd8WUFsKfbuJgMhiOEYyD5OwckLIjEwiHRQFoMIkMA1aqk2kMRX3l4Hf9cs4/JlW7mjC8hanOxqvaDetU3SMUgglFKCkwprHQmlCZcTCZIbTAcCZgeKhOJGERibdr00t8DLQhIKgifXSuUh285jRMnlmG3CRWFTl4ofDen5/0RWzTInauDtG7ZiDcUPTwlNI4hJpTpVFcTgzAYjgyMBZGJ6DAWxMAYBMD4kyCvgN48PUluYrkbZ55+tNVFLvYGC1hdcTkxJfx5c4xH3m6i3O1gfn1Zjm/k6OKUaRXMHV/CjBpTS8hgOBIwFkQmEpPiXMWAaAsiqSAyjG4d+fCJZ3hrQxQ27e9nGVQVu2j3hvi57XqmVJzGG7d+IPfyH6XMHlfC0lvPGm0xDAaDhbEgMpEIUue5IC9/QAxiCPfH+JNojxXisAv5jtRjrSpysq8nwMrmEMUzMywLajAYDEcoxoLIRLqCcOTr98NZEBaJ2dHpdYSqi110eHXbBZPNKl8Gg+HowVgQmUjWXbIsiEggLUg9vIIYOLehuigVs1gwySgIg8Fw9GAURCaiaSmtefk6qykLC8KTYXZ0laUgplS6qS426ZsGg+HowSiITKRXbnUUaAtiqGquaWSqr5RQCsa9ZDAYjjaMgshEYh5EXn6aBTFENdc0MrmYEuUjlkw59kt2GwyGYwsTpM5E+qQ4R0EqzdWWN2jlMW8oSigSo7LIhScYpXTA7Ohp1UX86eNLOG26WQbSYDAcXRgLIhOxkA5Qi6TSXKOhjJPkvvrIOm74w3KUUoPXeLA4+7jq1MJABoPBcJRgLIhMRMOpmkuOAujbr11MA8psBCMxXtjcCoAvHCMWV6aEt8FgOGYwCiIT0WBKQSQnyoUGFep7Y2cHwUgcgD2dPgBTX8lgMBwzGL9HJmLhlDvJkW/FICKDAtTPbWpLvk6sAWEUhMFgOFYwCiIT0VCqpEZeQVoMItX5x+OKFza3Ul+hK5Bu2e8BMIsAGQyGYwajIDIRC2WwIML9gtRv7+2mrS/E9adMBmDLfmNBGAyGY4ucKggRuUREtorIDhH5Wob9pSLyLxFZKyIbReTGtH27RWS9iKwRkZW5lHMQ0fBgCyIWTloQ+3uD3PrAGqqKnHx4cT0Ou7DZuJgMBsMxRs6C1CJiB24HLgSagBUi8rhSalPaYZ8BNimlrhCRamCriPxVKWVNROA8pVRHrmQckmhQB6chFZgOeWkLwKfueJ3m7gD+cIwHbj6VMreTcaX5NHYFAEwWk8FgOGbIpQWxBNihlGqwOvwHgPcOOEYBxaLLnxYBXUA0hzJlRyycCkg7dIyBYC/7vTGaugOcUFfKvR9bzAl1ekGhCaX6GBEozjeJYQaD4dgglwqiDmhMe99kbUvnN8AcYB+wHrhVKRW39ingWRFZJSI3D3UREblZRFaKyMr29vbDI3k01D/NFSDUizdm57ITxnHvxxazZGqqdEZiqcxiVx42mww8m8FgMByV5FJBZOop1YD3FwNrgAnAfOA3IlJi7TtDKbUAuBT4jIicnekiSqm7lVKLlFKLqqurD4vg/dNcdeevgh4CMTvjLWWQTqLekslgMhgMxxK5VBBNQH3a+4loSyGdG4F/KM0OYBcwG0Aptc/63wY8inZZjQwDJ8oBhDyEyUsqg3QSSsMEqA0Gw7FELhXECmCmiEwVESdwNfD4gGP2AucDiEgtMAtoEJFCESm2thcCFwEbcihrfwaW2gBExYmQl3QnpTPBUhpGQRgMhmOJnEVUlVJREfks8AxgB+5VSm0UkVus/XcB3wfuE5H1aJfUV5VSHSIyDXjUWrozD/ibUurpXMk6iFgoFaTOS1kMkaEsCCtIbTKYDAbDsURWCkJEHgHuBZ5KCyIfEKXUUmDpgG13pb3eh7YOBrZrAE7K9jqHnfQgtSNlMYTJo7ZksIKYUGYsCIPBcOyRrYvpTuBaYLuI/FhEZudQptEnUxYTkOfIz1i2u7TAwfjSfCZXuUdKQoPBYMg5WVkQSqnngedFpBS4BnhORBqB3wF/UUpFcijjyNOv1EbKgnC6BlsPACLCs188m3yHfSSkMxgMhhEh6yC1iFQCHwM+CawG/g9YADyXE8lGi1gUVDyjBZGfPzhAnaA432EWBTIYDMcU2cYg/oFOP/0zcIVSqsXa9fcRr5OUa2Ih/T9DkLpgGAVhMBgMxxrZZjH9Rin170w7lFKLDqM8o0/UUhAJxeBIKQi32ygIg8EwdsjWJzJHRMoSb0SkXEQ+nRuRRpmkgkir5mpRVGiC0AaDYeyQrYK4SSnVk3ijlOoGbsqJRKNN0sVkxSDsDpTox1TsNgrCYDCMHbJVEDar4iqQLOXtHOb4o5eoVWk8EaQWIWrTr4uLC0dJKIPBYBh5so1BPAM8KCJ3oQvu3QKM3MzmkSQa1P/T1p+OiAsHAQoLjAVhMBjGDtkqiK8CnwL+A10S41ngnlwJNarEEhZEWokNy4Kw57kytTAYDIZjkmwnysXRs6nvzK04RwADg9RAOOFNyzs2vWoGg8GQiWznQcwEfgTMBZJDa6XUtBzJNXoMDFIDIbEUg90oCIPBMHbINkj9B7T1EAXOA/6EnjR37JEMUqeUQVAZBWEwGMYe2SqIAqXUC4AopfYopb4LvCt3Yo0iiSB1WgwiiFEQBoNh7JFtkDooIjZ0NdfPAs1ATe7EGkUSQeo0F5M/bpXxNkFqg8EwhsjWgvgC4AY+DywErgduyJFMo0uGILVfWXrUbtZ7MBgMY4cDKghrUtyHlFJepVSTUupGpdSVSqk3s2h7iYhsFZEdIvK1DPtLReRfIrJWRDaKyI3Zts0ZGYLUvphj0DaDwWA41jmgglBKxYCF6TOps8FSLLcDl6Kzn64RkbkDDvsMsEkpdRJwLvBzEXFm2TY3JC2INAWRcDGZGITBYBhDZBuDWA38U0QeAnyJjUqpfwzTZgmww1o+FBF5AHgvsCntGAUUW8qnCOhCZ0qdkkXb3DBAQYSiMa0gbBgXk8FgGFNkqyAqgE76Zy4pYDgFUQc0pr1vQnf86fwGeBzYBxQDH1ZKxUUkm7YAiMjNwM0AkyZNOuCNHJABQWpvMEooOVHOuJgMBsPYIduZ1Dce+KhBZHJJqQHvLwbWoBXPdPRSpq9m2TYh293A3QCLFi3KeMxBEQ2BLQ9s2vvmC8UIYlxMBoNh7JHtTOo/kKGDVkp9fJhmTUB92vuJaEshnRuBHyulFLBDRHahV67Lpm1uCHT1WwOiLxShTxWgsCF5mdekNhgMhmORbNNcnwCetP5eAEoA7wHarABmishUEXECV6PdSensBc4HEJFaYBbQkGXbw0/QAxv+ATMvSG7yBqM8FDuHTe+6B5ymmqvBYBg7ZOtieiT9vYjcDzx/gDZRa1LdM4AduFcptVFEbrH23wV8H7hPRNaj3UpfVUp1WNcY1Pag7uxQWP1nCHng9M8lN3lDUTwUEZ16Rs4vbzAYDEcS2QapBzITOGBEWCm1FFg6YNtdaa/3ARdl2zanxCLw5p0w+QyoW5jc7A1FASh0HeqjMhgMhqOTbGMQffSPQexHrxFx7NC4HHob4aIf9NucUBDF+UZBGAyGsUW2LqbiXAsy6vja9f/KGf02e4NaQRQZC8JgMIwxsgpSi8j7RaQ07X2ZiLwvZ1KNBsFe/b+grN9mXyiKCLid9pGXyWAwGEaRbLOYvqOU6k28UUr1AN/JiUSjRbBH/88v7be5LxSlyJnHQVYaMRgMhqOebBVEpuOOLZ9LsBfEDs6ifpu9wShFJv5gMBjGINkqiJUi8gsRmS4i00Tkf4FVuRRsxAn0aPfSAEvBG4qa+IPBYBiTZKsgPgeEgb8DDwIBdCXWY4dgzyD3ElgKwlgQBoNhDJJtFpMPGLk1GUaDYC/klw3abCwIg8EwVsk2i+k5ESlLe18uIs/kTKrRINCT2YIIGgVhMBjGJtm6mKqszCUAlFLdHGtrUgd7B6W4grEgDAbD2CVbBREXkWRpDRGZwhDlt49aholBmDIbBoNhLJJtz/dN4DURedl6fzbWIj3HBEpljEEopfCGoqbMhsFgGJNkG6R+WkQWoZXCGuCf6EymY4NIQK8kN8DF5A/HUMqU2TAYDGOTbIv1fRK4Fb1wzxrgVGAZ/ZcgPXoZYha1zyrUZ9JcDQbDWCTbGMStwGJgj1LqPOBkoD1nUo00iTpMA1xMHlOoz2AwjGGyVRBBpVQQQERcSqkt6NXfjg0CPfr/AAui0xsCoLLQNcICGQwGw+iT7dC4yZoH8RjwnIh0M1JrRI8EQ1RybbcURFWxc4QFMhgMhtEn2yD1+62X3xWRF4FS4OkDtRORS4D/Qy8beo9S6scD9n8ZuC5NljlAtVKqS0R2A31ADIgqpRZlI+shkYxBlPXb3NGnFUR1kbEgDAbD2OOgnetKqZcPfBSIiB24HbgQaAJWiMjjSqlNaef6KfBT6/grgC8qpbrSTnNeYo3qnDJEDKLDG8ZuE8rdxoIwGAxjj2xjEIfCEmCHUqpBKRUGHgDeO8zx1wD351CeoRkiBtHeF6Ky0InNZtaCMBgMY49cKog6oDHtfZO1bRAi4gYuAR5J26yAZ0VklYgMOSlPRG4WkZUisrK9/RATq4K9eh0Ie3+DqsMbosq4lwwGwxgllwoi07B7qPIcVwCvD3AvnaGUWgBcCnxGRM7O1FApdbdSapFSalF1dfWhSRrsyVjJtcMboqrYKAiDwTA2yaWCaALq095PZOjMp6sZ4F5SSu2z/rcBj6JdVrlhiEqu7X0hE6A2GAxjllwqiBXATBGZKiJOtBJ4fOBBIlIKnIMu35HYVigixYnXwEXAhpxJmqGSq1KKDm/YpLgaDIYxS86mCCuloiLyWeAZdJrrvUqpjSJyi7X/LuvQ9wPPWosSJagFHhW9/Gce8Del1AHTag+ZYA+UTe63yROMEo7FjQVhMBjGLDmtIaGUWgosHbDtrgHv7wPuG7CtATgpl7L1I9ibMYMJoNrEIAwGwxglly6mo4dAzyAXU0diFrWxIAwGwxjFVKFTCs74PNQt6LfZKAiDwTDWMQpCBM75yqDNxsVkMBjGOsbFNAQd3hB2m1BW4BhtUQwGg2FUMApiCEyZDYPBMNYxCmIIOrxh414yGAxjGqMghsDUYTIYDGMdoyCGoL0vZCwIg8EwpjEKIgPxuKKtL0RtiVEQBoNh7GIURAY6fWFicUVtSf5oi2IwGAyjhlEQGWj1BAGoKTYKwmAwjF2MgshAW59WEMbFZDAYxjJGQWSg1aNnURsXk8FgGMsYBZGBhIvJZDEZDIaxjFEQGWj1hKgqcuKwm8djMBjGLqYHzECbJ2gC1AaDYcxjFEQGWvuCJkBtMBjGPDlVECJyiYhsFZEdIvK1DPu/LCJrrL8NIhITkYps2uaSVk/IBKgNBsOYJ2cKQkTswO3ApcBc4BoRmZt+jFLqp0qp+Uqp+cDXgZeVUl3ZtM0V0VicDm+IGqMgDAbDGCeXFsQSYIdSqkEpFQYeAN47zPHXAPcfYtvDRqcvjFJmDoTBYDDkUkHUAY1p75usbYMQETdwCfDIIbS9WURWisjK9vb2dyx0IsW11gSpDQbDGCeXCiLTSjtqiGOvAF5XSnUdbFul1N1KqUVKqUXV1dWHIGZ/zCQ5g8Fg0ORSQTQB9WnvJwL7hjj2alLupYNte1hJWhDGxWQwGMY4uVQQK4CZIjJVRJxoJfD4wINEpBQ4B/jnwbbNBW2eIDaBSrNYkMFgGOPk5erESqmoiHwWeAawA/cqpTaKyC3W/rusQ98PPKuU8h2oba5kTacnEKGkwIHdrEVtMBjGODlTEABKqaXA0gHb7hrw/j7gvmzajgSBcAy3wz7SlzUYDIYjDjOTegCBSIx8p1EQBoPBYBTEAIKRGAXGgjAYDAajIAbiDxsFYTAYDGAUxCACkRgFxsVkMBgMRkEMJGAsCIPBYACMghhE0FgQBoPBABgFMYiACVIbDAYDYBTEIALhGPlGQRgMBoNREAMxQWqDwWDQGAWRRiQWJxJTxsVkMBgMGAXRj2AkBmAUhMFgMGAURD8CCQVhXEwGg8FgFEQ6wXAcMBaEwWAwgFEQ/TAWhMFgMKQwCiKNgIlBGAwGQxKjINLwh6MAZh6EwWAwkGMFISKXiMhWEdkhIl8b4phzRWSNiGwUkZfTtu8WkfXWvpW5lDNB0LiYDAaDIUnOVpQTETtwO3Ah0ASsEJHHlVKb0o4pA+4ALlFK7RWRmgGnOU8p1ZErGQcSsILUbqMgDAaDIacWxBJgh1KqQSkVBh4A3jvgmGuBfyil9gIopdpyKM8BMTEIg8FgSJFLBVEHNKa9b7K2pXMcUC4iL4nIKhH5aNo+BTxrbb85h3ImSSgIE4MwGAyGHLqYAMmwTWW4/kLgfKAAWCYibyqltgFnKKX2WW6n50Rki1LqlUEX0crjZoBJkya9I4GDYRODMBgMhgS5tCCagPq09xOBfRmOeVop5bNiDa8AJwEopfZZ/9uAR9Euq0Eope5WSi1SSi2qrq5+RwL7LQWRn2eSuwwGgyGXPeEKYKaITBURJ3A18PiAY/4JnCUieSLiBk4BNotIoYgUA4hIIXARsCGHsgLaxeS028izGwVhMBgMOXMxKaWiIvJZ4BnADtyrlNooIrdY++9SSm0WkaeBdUAcuEcptUFEpgGPikhCxr8ppZ7OlawJgpEY+Q6jHAwGgwFyG4NAKbUUWDpg210D3v8U+OmAbQ1YrqaRJBCO4Xbm9JEYDAbDUYMZLqdhFgsyGAyGFEZBpBGImOVGDQaDIYFREGkEwjEKTAzCYDAYAKMg+mFcTAaDwZDCKIg0tAVhFITBYDCAURD9CJoYhMFgMCQxCiKNQCRmKrkaDAaDhVEQaQQixsVkMBgMCYyCSMMfjpFvLAiDwWAAjIJIEosrwtG4sSAMBoPBwigIi6BZLMhgMBj6MeYLDwXCMe58aQdTqgoBsxaEwWAwJBjzCsJhFx55uxmbZUuZNFeDwWDQjHkXU57dxifOnEpjVwDApLkaDAaDxZhXEAAfXlxPSb42pkwMwmAwGDRGQQCFrjyuP3UyYFxMBoPBkGDMxyAS3Hz2NELROCdPKhttUQwGg+GIIKcWhIhcIiJbRWSHiHxtiGPOFZE1IrJRRF4+mLaHkzK3k2+/e65ZUc5gMBgsctYbiogduB24EGgCVojI40qpTWnHlAF3AJcopfaKSE22bQ0Gg8GQW3JpQSwBdiilGpRSYeAB4L0DjrkW+IdSai+AUqrtINoaDAaDIYfkUkHUAY1p75usbekcB5SLyEsiskpEPnoQbQEQkZtFZKWIrGxvbz9MohsMBoMhlw53ybBNZbj+QuB8oABYJiJvZtlWb1TqbuBugEWLFmU8xmAwGAwHTy4VRBNQn/Z+IrAvwzEdSikf4BORV4CTsmxrMBgMhhySSxfTCmCmiEwVESdwNfD4gGP+CZwlInki4gZOATZn2dZgMBgMOSRnFoRSKioinwWeAezAvUqpjSJyi7X/LqXUZhF5GlgHxIF7lFIbADK1zZWsBoPBYBiMKHXsuO0XLVqkVq5cOdpiGAwGw1GDiKxSSi3KuO9YUhAi0g7sOcTmVUDHYRQnVxwtcsLRI+vRIiccPbIeLXLC0SNrruScrJSqzrTjmFIQ7wQRWTmUFj2SOFrkhKNH1qNFTjh6ZD1a5ISjR9bRkNMU6zMYDAZDRoyCMBgMBkNGjIJIcfdoC5AlR4uccPTIerTICUePrEeLnHD0yDricpoYhMFgMBgyYiwIg8FgMGTEKAiDwWAwZGTMK4iRXpjoYBCRehF5UUQ2Wwsq3Wpt/66INFsLLa0RkcuOAFl3i8h6S56V1rYKEXlORLZb/8uPADlnpT23NSLiEZEvHAnPVETuFZE2EdmQtm3IZygiX7e+t1tF5OIjQNafisgWEVknIo9a670gIlNEJJD2bO8aZTmH/KyPwGf69zQ5d4vIGmv7yDxTpdSY/UOX8dgJTAOcwFpg7mjLlSbfeGCB9boY2AbMBb4L3Dba8g2QdTdQNWDb/wBfs15/DfjJaMuZ4fPfD0w+Ep4pcDawANhwoGdofQ/WAi5gqvU9to+yrBcBedbrn6TJOiX9uCPgmWb8rI/EZzpg/8+B/xrJZzrWLYgjemEipVSLUupt63UfupBhxnUxjlDeC/zRev1H4H2jJ0pGzgd2KqUOdfb9YUUp9QrQNWDzUM/wvcADSqmQUmoXsAP9fR4RMsmqlHpWKRW13r6JrsI8qgzxTIfiiHumCUREgA8B94+UPGBcTFkvTDTaiMgU4GTgLWvTZy1T/t4jwXWDXq/jWWvhp5utbbVKqRbQyg6oGTXpMnM1/X9wR9ozhaGf4ZH+3f048FTa+6kislpEXhaRs0ZLqDQyfdZH8jM9C2hVSm1P25bzZzrWFUTWCxONJiJSBDwCfEEp5QHuBKYD84EWtOk52pyhlFoAXAp8RkTOHm2BhsMqI/8e4CFr05H4TIfjiP3uisg3gSjwV2tTCzBJKXUy8J/A30SkZLTkY+jP+oh9psA19B/MjMgzHesK4ohfmEhEHGjl8Fel1D8AlFKtSqmYUioO/I4RNIOHQim1z/rfBjyKlqlVRMYDWP/bhj7DiHMp8LZSqhWOzGdqMdQzPCK/uyJyA/Bu4DplOcstl02n9XoV2rd/3GjJOMxnfaQ+0zzgA8DfE9tG6pmOdQVxRC9MZPkdfw9sVkr9Im37+LTD3g9sGNh2JBGRQhEpTrxGBys3oJ/lDdZhN6AXiDpS6DciO9KeaRpDPcPHgatFxCUiU4GZwPJRkC+JiFwCfBV4j1LKn7a9WkTs1utpaFkbRkfKYT/rI+6ZWlwAbFFKNSU2jNgzHakI/ZH6B1yGzg7aCXxztOUZINuZaBN3HbDG+rsM+DOw3tr+ODB+lOWchs7+WAtsTDxHoBJ4Adhu/a8Y7WdqyeUGOoHStG2j/kzRCqsFiKBHs58Y7hkC37S+t1uBS48AWXegffiJ7+pd1rFXWt+LtcDbwBWjLOeQn/WR9kyt7fcBtww4dkSeqSm1YTAYDIaMjHUXk8FgMBiGwCgIg8FgMGTEKAiDwWAwZMQoCIPBYDBkxCgIg8FgMGTEKAiD4QhARM4VkSdGWw6DIR2jIAwGg8GQEaMgDIaDQESuF5HlVg3+34qIXUS8IvJzEXlbRF4QkWrr2Pki8mba+gjl1vYZIvK8iKy12ky3Tl8kIg9bayr81ZpJbzCMGkZBGAxZIiJzgA+jCxPOB2LAdUAhuq7TAuBl4DtWkz8BX1VKnYieuZvY/lfgdqXUScDp6NmzoKv1fgG9LsE04Iwc35LBMCx5oy2AwXAUcT6wEFhhDe4L0MXz4qQKqf0F+IeIlAJlSqmXre1/BB6yalbVKaUeBVBKBQGs8y1XVr0da+WwKcBrOb8rg2EIjIIwGLJHgD8qpb7eb6PItwccN1z9muHcRqG01zHM79MwyhgXk8GQPS8AV4lIDSTXi56M/h1dZR1zLfCaUqoX6E5byOUjwMtKr+fRJCLvs87hEhH3SN6EwZAtZoRiMGSJUmqTiHwLvXKeDV118zOADzheRFYBveg4Bejy3HdZCqABuNHa/hHgtyLyPescHxzB2zAYssZUczUY3iEi4lVKFY22HAbD4ca4mAwGg8GQEWNBGAwGgyEjxoIwGAwGQ0aMgjAYDAZDRoyCMBgMBkNGjIIwGAwGQ0aMgjAYDAZDRv4/Faa3VbMpuqoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e7db369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd10434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,0:-1]\n",
    "Y = data['size_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9e2ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "a = StandardScaler()\n",
    "a.fit(X)\n",
    "X_standardized = a.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a36b40a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.754024e-15</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-13.045818</td>\n",
       "      <td>-0.080635</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.408960</td>\n",
       "      <td>1.007353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>517.0</td>\n",
       "      <td>3.070830e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-1.715608</td>\n",
       "      <td>-0.660665</td>\n",
       "      <td>-0.040203</td>\n",
       "      <td>0.492739</td>\n",
       "      <td>2.819865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>517.0</td>\n",
       "      <td>7.387171e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-2.179108</td>\n",
       "      <td>-0.444828</td>\n",
       "      <td>0.469119</td>\n",
       "      <td>0.669663</td>\n",
       "      <td>1.261610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-3.865380e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-1.980578</td>\n",
       "      <td>-0.553595</td>\n",
       "      <td>-0.136477</td>\n",
       "      <td>0.390409</td>\n",
       "      <td>10.335381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>517.0</td>\n",
       "      <td>2.005703e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-2.876943</td>\n",
       "      <td>-0.584238</td>\n",
       "      <td>0.070821</td>\n",
       "      <td>0.674164</td>\n",
       "      <td>2.484195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>517.0</td>\n",
       "      <td>3.362881e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-1.796637</td>\n",
       "      <td>-0.692456</td>\n",
       "      <td>-0.140366</td>\n",
       "      <td>0.534411</td>\n",
       "      <td>3.417549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-2.676776e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-2.021098</td>\n",
       "      <td>-0.736124</td>\n",
       "      <td>-0.009834</td>\n",
       "      <td>0.492982</td>\n",
       "      <td>3.007063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-2.841054e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.073268</td>\n",
       "      <td>-0.073268</td>\n",
       "      <td>-0.073268</td>\n",
       "      <td>-0.073268</td>\n",
       "      <td>21.572284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.274502e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.202020</td>\n",
       "      <td>-0.202020</td>\n",
       "      <td>-0.193843</td>\n",
       "      <td>-0.098709</td>\n",
       "      <td>16.951110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>517.0</td>\n",
       "      <td>4.874674e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.443576</td>\n",
       "      <td>-0.443576</td>\n",
       "      <td>-0.443576</td>\n",
       "      <td>-0.443576</td>\n",
       "      <td>2.254407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.868267e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.408709</td>\n",
       "      <td>-0.408709</td>\n",
       "      <td>-0.408709</td>\n",
       "      <td>-0.408709</td>\n",
       "      <td>2.446730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-2.238699e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.440449</td>\n",
       "      <td>-0.440449</td>\n",
       "      <td>-0.440449</td>\n",
       "      <td>-0.440449</td>\n",
       "      <td>2.270410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-6.098711e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.474467</td>\n",
       "      <td>-0.474467</td>\n",
       "      <td>-0.474467</td>\n",
       "      <td>-0.474467</td>\n",
       "      <td>2.107630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.004999e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.365748</td>\n",
       "      <td>-0.365748</td>\n",
       "      <td>-0.365748</td>\n",
       "      <td>-0.365748</td>\n",
       "      <td>2.734120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>517.0</td>\n",
       "      <td>2.405125e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.375873</td>\n",
       "      <td>-0.375873</td>\n",
       "      <td>-0.375873</td>\n",
       "      <td>-0.375873</td>\n",
       "      <td>2.660475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-3.843906e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>2.928152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.344293e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>7.512952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>517.0</td>\n",
       "      <td>2.473843e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.743339</td>\n",
       "      <td>-0.743339</td>\n",
       "      <td>-0.743339</td>\n",
       "      <td>1.345282</td>\n",
       "      <td>1.345282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>517.0</td>\n",
       "      <td>7.179943e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>-0.133103</td>\n",
       "      <td>7.512952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-1.933764e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.200603</td>\n",
       "      <td>-0.200603</td>\n",
       "      <td>-0.200603</td>\n",
       "      <td>-0.200603</td>\n",
       "      <td>4.984977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-2.260174e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>16.046807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>517.0</td>\n",
       "      <td>1.352883e-17</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.256865</td>\n",
       "      <td>-0.256865</td>\n",
       "      <td>-0.256865</td>\n",
       "      <td>-0.256865</td>\n",
       "      <td>3.893103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>517.0</td>\n",
       "      <td>1.169277e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.184391</td>\n",
       "      <td>-0.184391</td>\n",
       "      <td>-0.184391</td>\n",
       "      <td>-0.184391</td>\n",
       "      <td>5.423261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>517.0</td>\n",
       "      <td>2.265542e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>-0.341512</td>\n",
       "      <td>2.928152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>517.0</td>\n",
       "      <td>-2.596515e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>-0.062318</td>\n",
       "      <td>16.046807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>517.0</td>\n",
       "      <td>1.443075e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.044023</td>\n",
       "      <td>-0.044023</td>\n",
       "      <td>-0.044023</td>\n",
       "      <td>-0.044023</td>\n",
       "      <td>22.715633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>517.0</td>\n",
       "      <td>6.253326e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.172860</td>\n",
       "      <td>-0.172860</td>\n",
       "      <td>-0.172860</td>\n",
       "      <td>-0.172860</td>\n",
       "      <td>5.785038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>517.0</td>\n",
       "      <td>4.024290e-16</td>\n",
       "      <td>1.000969</td>\n",
       "      <td>-0.706081</td>\n",
       "      <td>-0.706081</td>\n",
       "      <td>-0.706081</td>\n",
       "      <td>1.416268</td>\n",
       "      <td>1.416268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count          mean       std        min       25%       50%       75%  \\\n",
       "0   517.0 -1.754024e-15  1.000969 -13.045818 -0.080635  0.173229  0.408960   \n",
       "1   517.0  3.070830e-16  1.000969  -1.715608 -0.660665 -0.040203  0.492739   \n",
       "2   517.0  7.387171e-17  1.000969  -2.179108 -0.444828  0.469119  0.669663   \n",
       "3   517.0 -3.865380e-17  1.000969  -1.980578 -0.553595 -0.136477  0.390409   \n",
       "4   517.0  2.005703e-16  1.000969  -2.876943 -0.584238  0.070821  0.674164   \n",
       "5   517.0  3.362881e-16  1.000969  -1.796637 -0.692456 -0.140366  0.534411   \n",
       "6   517.0 -2.676776e-16  1.000969  -2.021098 -0.736124 -0.009834  0.492982   \n",
       "7   517.0 -2.841054e-16  1.000969  -0.073268 -0.073268 -0.073268 -0.073268   \n",
       "8   517.0 -1.274502e-16  1.000969  -0.202020 -0.202020 -0.193843 -0.098709   \n",
       "9   517.0  4.874674e-17  1.000969  -0.443576 -0.443576 -0.443576 -0.443576   \n",
       "10  517.0 -1.868267e-16  1.000969  -0.408709 -0.408709 -0.408709 -0.408709   \n",
       "11  517.0 -2.238699e-16  1.000969  -0.440449 -0.440449 -0.440449 -0.440449   \n",
       "12  517.0 -6.098711e-17  1.000969  -0.474467 -0.474467 -0.474467 -0.474467   \n",
       "13  517.0 -1.004999e-16  1.000969  -0.365748 -0.365748 -0.365748 -0.365748   \n",
       "14  517.0  2.405125e-17  1.000969  -0.375873 -0.375873 -0.375873 -0.375873   \n",
       "15  517.0 -3.843906e-17  1.000969  -0.341512 -0.341512 -0.341512 -0.341512   \n",
       "16  517.0 -1.344293e-16  1.000969  -0.133103 -0.133103 -0.133103 -0.133103   \n",
       "17  517.0  2.473843e-16  1.000969  -0.743339 -0.743339 -0.743339  1.345282   \n",
       "18  517.0  7.179943e-16  1.000969  -0.133103 -0.133103 -0.133103 -0.133103   \n",
       "19  517.0 -1.933764e-16  1.000969  -0.200603 -0.200603 -0.200603 -0.200603   \n",
       "20  517.0 -2.260174e-17  1.000969  -0.062318 -0.062318 -0.062318 -0.062318   \n",
       "21  517.0  1.352883e-17  1.000969  -0.256865 -0.256865 -0.256865 -0.256865   \n",
       "22  517.0  1.169277e-16  1.000969  -0.184391 -0.184391 -0.184391 -0.184391   \n",
       "23  517.0  2.265542e-16  1.000969  -0.341512 -0.341512 -0.341512 -0.341512   \n",
       "24  517.0 -2.596515e-16  1.000969  -0.062318 -0.062318 -0.062318 -0.062318   \n",
       "25  517.0  1.443075e-16  1.000969  -0.044023 -0.044023 -0.044023 -0.044023   \n",
       "26  517.0  6.253326e-16  1.000969  -0.172860 -0.172860 -0.172860 -0.172860   \n",
       "27  517.0  4.024290e-16  1.000969  -0.706081 -0.706081 -0.706081  1.416268   \n",
       "\n",
       "          max  \n",
       "0    1.007353  \n",
       "1    2.819865  \n",
       "2    1.261610  \n",
       "3   10.335381  \n",
       "4    2.484195  \n",
       "5    3.417549  \n",
       "6    3.007063  \n",
       "7   21.572284  \n",
       "8   16.951110  \n",
       "9    2.254407  \n",
       "10   2.446730  \n",
       "11   2.270410  \n",
       "12   2.107630  \n",
       "13   2.734120  \n",
       "14   2.660475  \n",
       "15   2.928152  \n",
       "16   7.512952  \n",
       "17   1.345282  \n",
       "18   7.512952  \n",
       "19   4.984977  \n",
       "20  16.046807  \n",
       "21   3.893103  \n",
       "22   5.423261  \n",
       "23   2.928152  \n",
       "24  16.046807  \n",
       "25  22.715633  \n",
       "26   5.785038  \n",
       "27   1.416268  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_standardized).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07fda759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning of Hyperparameters :- Batch Size and Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "225a71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "814b1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=28, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    \n",
    "    adam=Adam(lr=0.01)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03261c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_6444\\1903461034.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5; 1/9] START batch_size=10, epochs=10....................................\n",
      "[CV 1/5; 1/9] END .....batch_size=10, epochs=10;, score=0.971 total time=   1.3s\n",
      "[CV 2/5; 1/9] START batch_size=10, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/9] END .....batch_size=10, epochs=10;, score=0.846 total time=   1.2s\n",
      "[CV 3/5; 1/9] START batch_size=10, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/9] END .....batch_size=10, epochs=10;, score=0.913 total time=   1.5s\n",
      "[CV 4/5; 1/9] START batch_size=10, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/9] END .....batch_size=10, epochs=10;, score=0.883 total time=   1.5s\n",
      "[CV 5/5; 1/9] START batch_size=10, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/9] END .....batch_size=10, epochs=10;, score=0.854 total time=   2.0s\n",
      "[CV 1/5; 2/9] START batch_size=10, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/9] END .....batch_size=10, epochs=50;, score=0.971 total time=   2.8s\n",
      "[CV 2/5; 2/9] START batch_size=10, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/9] END .....batch_size=10, epochs=50;, score=0.856 total time=   3.3s\n",
      "[CV 3/5; 2/9] START batch_size=10, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/9] END .....batch_size=10, epochs=50;, score=0.874 total time=   3.8s\n",
      "[CV 4/5; 2/9] START batch_size=10, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 2/9] END .....batch_size=10, epochs=50;, score=0.913 total time=   4.8s\n",
      "[CV 5/5; 2/9] START batch_size=10, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 2/9] END .....batch_size=10, epochs=50;, score=0.893 total time=   3.0s\n",
      "[CV 1/5; 3/9] START batch_size=10, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 3/9] END ....batch_size=10, epochs=100;, score=1.000 total time=   5.6s\n",
      "[CV 2/5; 3/9] START batch_size=10, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/9] END ....batch_size=10, epochs=100;, score=0.846 total time=   8.0s\n",
      "[CV 3/5; 3/9] START batch_size=10, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 3/9] END ....batch_size=10, epochs=100;, score=0.883 total time=   5.6s\n",
      "[CV 4/5; 3/9] START batch_size=10, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/9] END ....batch_size=10, epochs=100;, score=0.913 total time=   9.1s\n",
      "[CV 5/5; 3/9] START batch_size=10, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/9] END ....batch_size=10, epochs=100;, score=0.883 total time=   5.2s\n",
      "[CV 1/5; 4/9] START batch_size=20, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 4/9] END .....batch_size=20, epochs=10;, score=0.990 total time=   1.2s\n",
      "[CV 2/5; 4/9] START batch_size=20, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/9] END .....batch_size=20, epochs=10;, score=0.846 total time=   1.0s\n",
      "[CV 3/5; 4/9] START batch_size=20, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 4/9] END .....batch_size=20, epochs=10;, score=0.816 total time=   0.9s\n",
      "[CV 4/5; 4/9] START batch_size=20, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/9] END .....batch_size=20, epochs=10;, score=0.893 total time=   1.7s\n",
      "[CV 5/5; 4/9] START batch_size=20, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 4/9] END .....batch_size=20, epochs=10;, score=0.854 total time=   2.3s\n",
      "[CV 1/5; 5/9] START batch_size=20, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 5/9] END .....batch_size=20, epochs=50;, score=1.000 total time=   2.9s\n",
      "[CV 2/5; 5/9] START batch_size=20, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/9] END .....batch_size=20, epochs=50;, score=0.894 total time=   2.0s\n",
      "[CV 3/5; 5/9] START batch_size=20, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 5/9] END .....batch_size=20, epochs=50;, score=0.883 total time=   1.7s\n",
      "[CV 4/5; 5/9] START batch_size=20, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/9] END .....batch_size=20, epochs=50;, score=0.922 total time=   2.2s\n",
      "[CV 5/5; 5/9] START batch_size=20, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/9] END .....batch_size=20, epochs=50;, score=0.883 total time=   1.9s\n",
      "[CV 1/5; 6/9] START batch_size=20, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/9] END ....batch_size=20, epochs=100;, score=0.962 total time=   5.0s\n",
      "[CV 2/5; 6/9] START batch_size=20, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/9] END ....batch_size=20, epochs=100;, score=0.846 total time=   4.2s\n",
      "[CV 3/5; 6/9] START batch_size=20, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/9] END ....batch_size=20, epochs=100;, score=0.883 total time=   3.4s\n",
      "[CV 4/5; 6/9] START batch_size=20, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/9] END ....batch_size=20, epochs=100;, score=0.922 total time=   3.2s\n",
      "[CV 5/5; 6/9] START batch_size=20, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 6/9] END ....batch_size=20, epochs=100;, score=0.883 total time=   5.2s\n",
      "[CV 1/5; 7/9] START batch_size=40, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 7/9] END .....batch_size=40, epochs=10;, score=0.981 total time=   1.7s\n",
      "[CV 2/5; 7/9] START batch_size=40, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 7/9] END .....batch_size=40, epochs=10;, score=0.760 total time=   1.0s\n",
      "[CV 3/5; 7/9] START batch_size=40, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 7/9] END .....batch_size=40, epochs=10;, score=0.524 total time=   0.9s\n",
      "[CV 4/5; 7/9] START batch_size=40, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_test_function.<locals>.test_function at 0x0000027DF2997DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 7/9] END .....batch_size=40, epochs=10;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 7/9] START batch_size=40, epochs=10....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000027DF5415CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 7/9] END .....batch_size=40, epochs=10;, score=0.806 total time=   0.8s\n",
      "[CV 1/5; 8/9] START batch_size=40, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 8/9] END .....batch_size=40, epochs=50;, score=1.000 total time=   1.4s\n",
      "[CV 2/5; 8/9] START batch_size=40, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 8/9] END .....batch_size=40, epochs=50;, score=0.846 total time=   1.7s\n",
      "[CV 3/5; 8/9] START batch_size=40, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/9] END .....batch_size=40, epochs=50;, score=0.883 total time=   1.4s\n",
      "[CV 4/5; 8/9] START batch_size=40, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 8/9] END .....batch_size=40, epochs=50;, score=0.913 total time=   2.3s\n",
      "[CV 5/5; 8/9] START batch_size=40, epochs=50....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 8/9] END .....batch_size=40, epochs=50;, score=0.864 total time=   2.8s\n",
      "[CV 1/5; 9/9] START batch_size=40, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 9/9] END ....batch_size=40, epochs=100;, score=1.000 total time=   3.0s\n",
      "[CV 2/5; 9/9] START batch_size=40, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 9/9] END ....batch_size=40, epochs=100;, score=0.875 total time=   2.0s\n",
      "[CV 3/5; 9/9] START batch_size=40, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/9] END ....batch_size=40, epochs=100;, score=0.893 total time=   1.9s\n",
      "[CV 4/5; 9/9] START batch_size=40, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 9/9] END ....batch_size=40, epochs=100;, score=0.883 total time=   2.4s\n",
      "[CV 5/5; 9/9] START batch_size=40, epochs=100...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 9/9] END ....batch_size=40, epochs=100;, score=0.903 total time=   2.8s\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
    "# Define the grid search parameters\n",
    "batch_size = [10,20,40]\n",
    "epochs = [10,50,100]\n",
    "# Make a dictionary of the grid search parameters\n",
    "param_grid = dict(batch_size = batch_size,epochs = epochs)\n",
    "# Build and fit the GridSearchCV\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grid,verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc88805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.9167102336883545, using {'batch_size': 20, 'epochs': 50}\n",
      "0.8935586333274841,0.0453182669810321 with: {'batch_size': 10, 'epochs': 10}\n",
      "0.901306939125061,0.039757598010894374 with: {'batch_size': 10, 'epochs': 50}\n",
      "0.9051531076431274,0.051905209613516204 with: {'batch_size': 10, 'epochs': 100}\n",
      "0.8799290537834168,0.06052679441874298 with: {'batch_size': 20, 'epochs': 10}\n",
      "0.9167102336883545,0.04400131104576081 with: {'batch_size': 20, 'epochs': 50}\n",
      "0.8994025349617004,0.03931425667666764 with: {'batch_size': 20, 'epochs': 100}\n",
      "0.7500186562538147,0.14991401857013623 with: {'batch_size': 40, 'epochs': 10}\n",
      "0.9012696146965027,0.05406194494163243 with: {'batch_size': 40, 'epochs': 50}\n",
      "0.9109223246574402,0.04550954857400078 with: {'batch_size': 40, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01350fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning of Hyperparameters:- Learning rate and Drop out rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3922a3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_6444\\3315804937.py:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n",
      "[CV 1/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.971 total time=   0.9s\n",
      "[CV 2/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.750 total time=   1.1s\n",
      "[CV 3/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.534 total time=   0.9s\n",
      "[CV 4/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.680 total time=   1.8s\n",
      "[CV 5/5; 1/9] START dropout_rate=0.0, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/9] END dropout_rate=0.0, learning_rate=0.001;, score=0.699 total time=   1.3s\n",
      "[CV 1/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.981 total time=   1.1s\n",
      "[CV 2/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.769 total time=   0.9s\n",
      "[CV 3/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.680 total time=   1.1s\n",
      "[CV 4/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.718 total time=   0.8s\n",
      "[CV 5/5; 2/9] START dropout_rate=0.0, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 2/9] END dropout_rate=0.0, learning_rate=0.01;, score=0.786 total time=   0.9s\n",
      "[CV 1/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=1.000 total time=   0.8s\n",
      "[CV 2/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.808 total time=   0.9s\n",
      "[CV 3/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.524 total time=   2.1s\n",
      "[CV 4/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.757 total time=   2.3s\n",
      "[CV 5/5; 3/9] START dropout_rate=0.0, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/9] END dropout_rate=0.0, learning_rate=0.1;, score=0.864 total time=   1.7s\n",
      "[CV 1/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=1.000 total time=   0.9s\n",
      "[CV 2/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.750 total time=   1.4s\n",
      "[CV 3/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.524 total time=   1.0s\n",
      "[CV 4/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.680 total time=   1.0s\n",
      "[CV 5/5; 4/9] START dropout_rate=0.1, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 4/9] END dropout_rate=0.1, learning_rate=0.001;, score=0.699 total time=   1.1s\n",
      "[CV 1/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.942 total time=   1.3s\n",
      "[CV 2/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.750 total time=   1.1s\n",
      "[CV 3/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.524 total time=   0.9s\n",
      "[CV 4/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.748 total time=   2.1s\n",
      "[CV 5/5; 5/9] START dropout_rate=0.1, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/9] END dropout_rate=0.1, learning_rate=0.01;, score=0.786 total time=   2.3s\n",
      "[CV 1/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=1.000 total time=   1.9s\n",
      "[CV 2/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.760 total time=   0.9s\n",
      "[CV 3/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.524 total time=   1.0s\n",
      "[CV 4/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.825 total time=   1.2s\n",
      "[CV 5/5; 6/9] START dropout_rate=0.1, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 6/9] END dropout_rate=0.1, learning_rate=0.1;, score=0.699 total time=   0.9s\n",
      "[CV 1/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=1.000 total time=   1.0s\n",
      "[CV 2/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.750 total time=   1.0s\n",
      "[CV 3/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.524 total time=   1.2s\n",
      "[CV 4/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.680 total time=   1.0s\n",
      "[CV 5/5; 7/9] START dropout_rate=0.2, learning_rate=0.001.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 7/9] END dropout_rate=0.2, learning_rate=0.001;, score=0.709 total time=   1.3s\n",
      "[CV 1/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.990 total time=   2.4s\n",
      "[CV 2/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.750 total time=   2.4s\n",
      "[CV 3/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.660 total time=   1.0s\n",
      "[CV 4/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.767 total time=   1.0s\n",
      "[CV 5/5; 8/9] START dropout_rate=0.2, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 8/9] END dropout_rate=0.2, learning_rate=0.01;, score=0.699 total time=   1.1s\n",
      "[CV 1/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.990 total time=   1.3s\n",
      "[CV 2/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.750 total time=   1.0s\n",
      "[CV 3/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.524 total time=   1.1s\n",
      "[CV 4/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.680 total time=   1.0s\n",
      "[CV 5/5; 9/9] START dropout_rate=0.2, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 9/9] END dropout_rate=0.2, learning_rate=0.1;, score=0.699 total time=   1.0s\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Defining the model\n",
    "\n",
    "def create_model(learning_rate,dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8,input_dim = 28,kernel_initializer = 'normal',activation = 'relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(4,input_dim = 28,kernel_initializer = 'normal',activation = 'relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = learning_rate)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
    "\n",
    "# Define the grid search parameters\n",
    "\n",
    "learning_rate = [0.001,0.01,0.1]\n",
    "dropout_rate = [0.0,0.1,0.2]\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "\n",
    "param_grids = dict(learning_rate = learning_rate,dropout_rate = dropout_rate)\n",
    "\n",
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83eefed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.7906646728515625, using {'dropout_rate': 0.0, 'learning_rate': 0.1}\n",
      "0.7267550468444824,0.1417210764724635 with: {'dropout_rate': 0.0, 'learning_rate': 0.001}\n",
      "0.7868932008743286,0.10401382470945718 with: {'dropout_rate': 0.0, 'learning_rate': 0.01}\n",
      "0.7906646728515625,0.15593605185038814 with: {'dropout_rate': 0.0, 'learning_rate': 0.1}\n",
      "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.1, 'learning_rate': 0.001}\n",
      "0.7501120328903198,0.13361647289250625 with: {'dropout_rate': 0.1, 'learning_rate': 0.01}\n",
      "0.7616318106651306,0.1556461699480205 with: {'dropout_rate': 0.1, 'learning_rate': 0.1}\n",
      "0.7325242638587952,0.15400213076804797 with: {'dropout_rate': 0.2, 'learning_rate': 0.001}\n",
      "0.7733196496963501,0.11490651104685592 with: {'dropout_rate': 0.2, 'learning_rate': 0.01}\n",
      "0.728659451007843,0.1510055827692834 with: {'dropout_rate': 0.2, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f91c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning of Hyperparameters:- Activation Function and Kernel Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "481d8ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_6444\\3991236647.py:18: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 1/5; 1/12] START activation_function=softmax, init=uniform..................\n",
      "[CV 1/5; 1/12] END activation_function=softmax, init=uniform;, score=0.000 total time=   1.1s\n",
      "[CV 2/5; 1/12] START activation_function=softmax, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/12] END activation_function=softmax, init=uniform;, score=0.750 total time=   1.6s\n",
      "[CV 3/5; 1/12] START activation_function=softmax, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/12] END activation_function=softmax, init=uniform;, score=0.476 total time=   2.1s\n",
      "[CV 4/5; 1/12] START activation_function=softmax, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/12] END activation_function=softmax, init=uniform;, score=0.680 total time=   1.6s\n",
      "[CV 5/5; 1/12] START activation_function=softmax, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/12] END activation_function=softmax, init=uniform;, score=0.699 total time=   0.9s\n",
      "[CV 1/5; 2/12] START activation_function=softmax, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/12] END activation_function=softmax, init=normal;, score=1.000 total time=   0.9s\n",
      "[CV 2/5; 2/12] START activation_function=softmax, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/12] END activation_function=softmax, init=normal;, score=0.250 total time=   0.9s\n",
      "[CV 3/5; 2/12] START activation_function=softmax, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/12] END activation_function=softmax, init=normal;, score=0.524 total time=   1.3s\n",
      "[CV 4/5; 2/12] START activation_function=softmax, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 2/12] END activation_function=softmax, init=normal;, score=0.680 total time=   0.9s\n",
      "[CV 5/5; 2/12] START activation_function=softmax, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 2/12] END activation_function=softmax, init=normal;, score=0.699 total time=   1.0s\n",
      "[CV 1/5; 3/12] START activation_function=softmax, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 3/12] END activation_function=softmax, init=zero;, score=0.000 total time=   1.2s\n",
      "[CV 2/5; 3/12] START activation_function=softmax, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/12] END activation_function=softmax, init=zero;, score=0.750 total time=   2.6s\n",
      "[CV 3/5; 3/12] START activation_function=softmax, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 3/12] END activation_function=softmax, init=zero;, score=0.524 total time=   2.1s\n",
      "[CV 4/5; 3/12] START activation_function=softmax, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/12] END activation_function=softmax, init=zero;, score=0.680 total time=   1.4s\n",
      "[CV 5/5; 3/12] START activation_function=softmax, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/12] END activation_function=softmax, init=zero;, score=0.301 total time=   1.0s\n",
      "[CV 1/5; 4/12] START activation_function=relu, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 4/12] END activation_function=relu, init=uniform;, score=1.000 total time=   1.3s\n",
      "[CV 2/5; 4/12] START activation_function=relu, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/12] END activation_function=relu, init=uniform;, score=0.750 total time=   0.9s\n",
      "[CV 3/5; 4/12] START activation_function=relu, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 4/12] END activation_function=relu, init=uniform;, score=0.524 total time=   0.9s\n",
      "[CV 4/5; 4/12] START activation_function=relu, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/12] END activation_function=relu, init=uniform;, score=0.680 total time=   1.0s\n",
      "[CV 5/5; 4/12] START activation_function=relu, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 4/12] END activation_function=relu, init=uniform;, score=0.709 total time=   1.1s\n",
      "[CV 1/5; 5/12] START activation_function=relu, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 5/12] END activation_function=relu, init=normal;, score=0.990 total time=   0.8s\n",
      "[CV 2/5; 5/12] START activation_function=relu, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/12] END activation_function=relu, init=normal;, score=0.750 total time=   1.0s\n",
      "[CV 3/5; 5/12] START activation_function=relu, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 5/12] END activation_function=relu, init=normal;, score=0.524 total time=   1.6s\n",
      "[CV 4/5; 5/12] START activation_function=relu, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/12] END activation_function=relu, init=normal;, score=0.680 total time=   2.9s\n",
      "[CV 5/5; 5/12] START activation_function=relu, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/12] END activation_function=relu, init=normal;, score=0.689 total time=   2.3s\n",
      "[CV 1/5; 6/12] START activation_function=relu, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/12] END activation_function=relu, init=zero;, score=1.000 total time=   0.9s\n",
      "[CV 2/5; 6/12] START activation_function=relu, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/12] END activation_function=relu, init=zero;, score=0.750 total time=   0.9s\n",
      "[CV 3/5; 6/12] START activation_function=relu, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/12] END activation_function=relu, init=zero;, score=0.524 total time=   1.0s\n",
      "[CV 4/5; 6/12] START activation_function=relu, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/12] END activation_function=relu, init=zero;, score=0.680 total time=   1.2s\n",
      "[CV 5/5; 6/12] START activation_function=relu, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 6/12] END activation_function=relu, init=zero;, score=0.699 total time=   0.9s\n",
      "[CV 1/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 7/12] END activation_function=tanh, init=uniform;, score=0.952 total time=   1.0s\n",
      "[CV 2/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 7/12] END activation_function=tanh, init=uniform;, score=0.750 total time=   0.8s\n",
      "[CV 3/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 7/12] END activation_function=tanh, init=uniform;, score=0.612 total time=   1.1s\n",
      "[CV 4/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 7/12] END activation_function=tanh, init=uniform;, score=0.680 total time=   1.0s\n",
      "[CV 5/5; 7/12] START activation_function=tanh, init=uniform.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 7/12] END activation_function=tanh, init=uniform;, score=0.718 total time=   1.9s\n",
      "[CV 1/5; 8/12] START activation_function=tanh, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 8/12] END activation_function=tanh, init=normal;, score=0.990 total time=   2.0s\n",
      "[CV 2/5; 8/12] START activation_function=tanh, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 8/12] END activation_function=tanh, init=normal;, score=0.750 total time=   2.0s\n",
      "[CV 3/5; 8/12] START activation_function=tanh, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/12] END activation_function=tanh, init=normal;, score=0.563 total time=   0.9s\n",
      "[CV 4/5; 8/12] START activation_function=tanh, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 8/12] END activation_function=tanh, init=normal;, score=0.689 total time=   0.9s\n",
      "[CV 5/5; 8/12] START activation_function=tanh, init=normal......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 8/12] END activation_function=tanh, init=normal;, score=0.709 total time=   1.0s\n",
      "[CV 1/5; 9/12] START activation_function=tanh, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 9/12] END activation_function=tanh, init=zero;, score=1.000 total time=   0.9s\n",
      "[CV 2/5; 9/12] START activation_function=tanh, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 9/12] END activation_function=tanh, init=zero;, score=0.750 total time=   1.5s\n",
      "[CV 3/5; 9/12] START activation_function=tanh, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/12] END activation_function=tanh, init=zero;, score=0.524 total time=   1.0s\n",
      "[CV 4/5; 9/12] START activation_function=tanh, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 9/12] END activation_function=tanh, init=zero;, score=0.680 total time=   0.8s\n",
      "[CV 5/5; 9/12] START activation_function=tanh, init=zero........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 9/12] END activation_function=tanh, init=zero;, score=0.699 total time=   0.9s\n",
      "[CV 1/5; 10/12] START activation_function=linear, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 10/12] END activation_function=linear, init=uniform;, score=0.942 total time=   1.3s\n",
      "[CV 2/5; 10/12] START activation_function=linear, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 10/12] END activation_function=linear, init=uniform;, score=0.750 total time=   1.9s\n",
      "[CV 3/5; 10/12] START activation_function=linear, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 10/12] END activation_function=linear, init=uniform;, score=0.602 total time=   2.0s\n",
      "[CV 4/5; 10/12] START activation_function=linear, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 10/12] END activation_function=linear, init=uniform;, score=0.680 total time=   1.8s\n",
      "[CV 5/5; 10/12] START activation_function=linear, init=uniform..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 10/12] END activation_function=linear, init=uniform;, score=0.709 total time=   1.1s\n",
      "[CV 1/5; 11/12] START activation_function=linear, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 11/12] END activation_function=linear, init=normal;, score=0.971 total time=   0.9s\n",
      "[CV 2/5; 11/12] START activation_function=linear, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 11/12] END activation_function=linear, init=normal;, score=0.750 total time=   1.0s\n",
      "[CV 3/5; 11/12] START activation_function=linear, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 11/12] END activation_function=linear, init=normal;, score=0.602 total time=   0.9s\n",
      "[CV 4/5; 11/12] START activation_function=linear, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 11/12] END activation_function=linear, init=normal;, score=0.689 total time=   1.1s\n",
      "[CV 5/5; 11/12] START activation_function=linear, init=normal...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 11/12] END activation_function=linear, init=normal;, score=0.699 total time=   1.0s\n",
      "[CV 1/5; 12/12] START activation_function=linear, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 12/12] END activation_function=linear, init=zero;, score=1.000 total time=   0.8s\n",
      "[CV 2/5; 12/12] START activation_function=linear, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 12/12] END activation_function=linear, init=zero;, score=0.750 total time=   0.9s\n",
      "[CV 3/5; 12/12] START activation_function=linear, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 12/12] END activation_function=linear, init=zero;, score=0.524 total time=   0.9s\n",
      "[CV 4/5; 12/12] START activation_function=linear, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 12/12] END activation_function=linear, init=zero;, score=0.680 total time=   2.1s\n",
      "[CV 5/5; 12/12] START activation_function=linear, init=zero.....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 12/12] END activation_function=linear, init=zero;, score=0.699 total time=   1.9s\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "# Defining the model\n",
    "\n",
    "def create_model(activation_function,init):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(4,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = 0.001)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
    "\n",
    "# Define the grid search parameters\n",
    "activation_function = ['softmax','relu','tanh','linear']\n",
    "init = ['uniform','normal','zero']\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "param_grids = dict(activation_function = activation_function,init = init)\n",
    "\n",
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ca6420f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.7423263549804687, using {'activation_function': 'tanh', 'init': 'uniform'}\n",
      "0.5208737850189209,0.2766888088877092 with: {'activation_function': 'softmax', 'init': 'uniform'}\n",
      "0.6305825233459472,0.24482772813004766 with: {'activation_function': 'softmax', 'init': 'normal'}\n",
      "0.45097087025642396,0.27310905134889035 with: {'activation_function': 'softmax', 'init': 'zero'}\n",
      "0.7325242638587952,0.15400213076804797 with: {'activation_function': 'relu', 'init': 'uniform'}\n",
      "0.7267176985740662,0.15143591735437695 with: {'activation_function': 'relu', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'relu', 'init': 'zero'}\n",
      "0.7423263549804687,0.11451570735468528 with: {'activation_function': 'tanh', 'init': 'uniform'}\n",
      "0.7403099298477173,0.13973470943929384 with: {'activation_function': 'tanh', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'tanh', 'init': 'zero'}\n",
      "0.7365197896957397,0.11371192105575179 with: {'activation_function': 'linear', 'init': 'uniform'}\n",
      "0.7422890305519104,0.12394596970524079 with: {'activation_function': 'linear', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'linear', 'init': 'zero'}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbd88d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning of Hyperparameter :-Number of Neurons in activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d11cf074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_6444\\2663671110.py:17: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5; 1/9] START neuron1=4, neuron2=2........................................\n",
      "[CV 1/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.962 total time=   1.1s\n",
      "[CV 2/5; 1/9] START neuron1=4, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.750 total time=   1.5s\n",
      "[CV 3/5; 1/9] START neuron1=4, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.544 total time=   1.5s\n",
      "[CV 4/5; 1/9] START neuron1=4, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.680 total time=   1.3s\n",
      "[CV 5/5; 1/9] START neuron1=4, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/9] END .........neuron1=4, neuron2=2;, score=0.718 total time=   0.9s\n",
      "[CV 1/5; 2/9] START neuron1=4, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.981 total time=   1.1s\n",
      "[CV 2/5; 2/9] START neuron1=4, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.750 total time=   0.9s\n",
      "[CV 3/5; 2/9] START neuron1=4, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.583 total time=   2.2s\n",
      "[CV 4/5; 2/9] START neuron1=4, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.680 total time=   2.1s\n",
      "[CV 5/5; 2/9] START neuron1=4, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 2/9] END .........neuron1=4, neuron2=4;, score=0.718 total time=   2.4s\n",
      "[CV 1/5; 3/9] START neuron1=4, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 3/9] END .........neuron1=4, neuron2=8;, score=1.000 total time=   1.0s\n",
      "[CV 2/5; 3/9] START neuron1=4, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.750 total time=   0.9s\n",
      "[CV 3/5; 3/9] START neuron1=4, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.583 total time=   1.0s\n",
      "[CV 4/5; 3/9] START neuron1=4, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.680 total time=   1.1s\n",
      "[CV 5/5; 3/9] START neuron1=4, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/9] END .........neuron1=4, neuron2=8;, score=0.718 total time=   1.3s\n",
      "[CV 1/5; 4/9] START neuron1=8, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.981 total time=   0.9s\n",
      "[CV 2/5; 4/9] START neuron1=8, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.750 total time=   1.1s\n",
      "[CV 3/5; 4/9] START neuron1=8, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.621 total time=   0.9s\n",
      "[CV 4/5; 4/9] START neuron1=8, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.680 total time=   2.0s\n",
      "[CV 5/5; 4/9] START neuron1=8, neuron2=2........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 4/9] END .........neuron1=8, neuron2=2;, score=0.699 total time=   1.9s\n",
      "[CV 1/5; 5/9] START neuron1=8, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.962 total time=   2.0s\n",
      "[CV 2/5; 5/9] START neuron1=8, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.750 total time=   1.1s\n",
      "[CV 3/5; 5/9] START neuron1=8, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.612 total time=   1.3s\n",
      "[CV 4/5; 5/9] START neuron1=8, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.680 total time=   0.9s\n",
      "[CV 5/5; 5/9] START neuron1=8, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/9] END .........neuron1=8, neuron2=4;, score=0.699 total time=   0.9s\n",
      "[CV 1/5; 6/9] START neuron1=8, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.942 total time=   0.9s\n",
      "[CV 2/5; 6/9] START neuron1=8, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.750 total time=   1.1s\n",
      "[CV 3/5; 6/9] START neuron1=8, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.612 total time=   0.8s\n",
      "[CV 4/5; 6/9] START neuron1=8, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.689 total time=   1.0s\n",
      "[CV 5/5; 6/9] START neuron1=8, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 6/9] END .........neuron1=8, neuron2=8;, score=0.699 total time=   0.9s\n",
      "[CV 1/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.962 total time=   1.7s\n",
      "[CV 2/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.750 total time=   2.6s\n",
      "[CV 3/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.602 total time=   1.4s\n",
      "[CV 4/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.689 total time=   0.9s\n",
      "[CV 5/5; 7/9] START neuron1=16, neuron2=2.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 7/9] END ........neuron1=16, neuron2=2;, score=0.689 total time=   0.9s\n",
      "[CV 1/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.933 total time=   1.4s\n",
      "[CV 2/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.750 total time=   0.9s\n",
      "[CV 3/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.650 total time=   0.9s\n",
      "[CV 4/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.699 total time=   1.0s\n",
      "[CV 5/5; 8/9] START neuron1=16, neuron2=4.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 8/9] END ........neuron1=16, neuron2=4;, score=0.699 total time=   1.0s\n",
      "[CV 1/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.962 total time=   0.9s\n",
      "[CV 2/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.750 total time=   0.9s\n",
      "[CV 3/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.641 total time=   1.1s\n",
      "[CV 4/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.689 total time=   2.3s\n",
      "[CV 5/5; 9/9] START neuron1=16, neuron2=8.......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 9/9] END ........neuron1=16, neuron2=8;, score=0.718 total time=   1.9s\n"
     ]
    }
   ],
   "source": [
    "# Defining the model\n",
    "\n",
    "def create_model(neuron1,neuron2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron1,input_dim = 28,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = 0.001)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
    "\n",
    "# Define the grid search parameters\n",
    "\n",
    "neuron1 = [4,8,16]\n",
    "neuron2 = [2,4,8]\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "\n",
    "param_grids = dict(neuron1 = neuron1,neuron2 = neuron2)\n",
    "\n",
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6b240d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.7520164251327515, using {'neuron1': 16, 'neuron2': 8}\n",
      "0.7306571960449219,0.13519765469567385 with: {'neuron1': 4, 'neuron2': 2}\n",
      "0.7422703504562378,0.13187414013083615 with: {'neuron1': 4, 'neuron2': 4}\n",
      "0.7461165070533753,0.13886888958859903 with: {'neuron1': 4, 'neuron2': 8}\n",
      "0.7461538434028625,0.12432334151520222 with: {'neuron1': 8, 'neuron2': 2}\n",
      "0.7403659343719482,0.11914493496620072 with: {'neuron1': 8, 'neuron2': 4}\n",
      "0.7384615421295166,0.11111904628923971 with: {'neuron1': 8, 'neuron2': 8}\n",
      "0.7384241938591003,0.12113115620435422 with: {'neuron1': 16, 'neuron2': 2}\n",
      "0.7462472081184387,0.09839233243248231 with: {'neuron1': 16, 'neuron2': 4}\n",
      "0.7520164251327515,0.11075697876455771 with: {'neuron1': 16, 'neuron2': 8}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f62f148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training model with optimum values of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c2b886e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_6444\\402875659.py:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 3ms/step\n",
      "0.7775628626692457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Defining the model\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16,input_dim = 28,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(4,input_dim = 16,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = 0.001) #sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n",
    "\n",
    "# Fitting the model\n",
    "\n",
    "model.fit(X_standardized,Y)\n",
    "\n",
    "# Predicting using trained model\n",
    "\n",
    "y_predict = model.predict(X_standardized)\n",
    "\n",
    "# Printing the metrics\n",
    "print(accuracy_score(Y,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba171b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0923b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate,dropout_rate,activation_function,init,neuron1,neuron2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron1,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = learning_rate)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95d46b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_6444\\2849873856.py:3: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn = create_model,verbose = 0)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1f828cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid search parameters\n",
    "\n",
    "batch_size = [10,20,40]\n",
    "epochs = [10,50,100]\n",
    "learning_rate = [0.001,0.01,0.1]\n",
    "dropout_rate = [0.0,0.1,0.2]\n",
    "activation_function = ['softmax','relu','tanh','linear']\n",
    "init = ['uniform','normal','zero']\n",
    "neuron1 = [4,8,16]\n",
    "neuron2 = [2,4,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aee70ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of the grid search parameters\n",
    "\n",
    "param_grids = dict(batch_size = batch_size,epochs = epochs,learning_rate = learning_rate,dropout_rate = dropout_rate,\n",
    "                   activation_function = activation_function,init = init,neuron1 = neuron1,neuron2 = neuron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f3200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8748 candidates, totalling 43740 fits\n",
      "[CV 1/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=1.000 total time=   1.7s\n",
      "[CV 2/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.750 total time=   1.6s\n",
      "[CV 3/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.524 total time=   1.8s\n",
      "[CV 4/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.680 total time=   1.6s\n",
      "[CV 5/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.699 total time=   2.0s\n",
      "[CV 1/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=1.000 total time=   1.4s\n",
      "[CV 2/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.750 total time=   2.2s\n",
      "[CV 3/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.524 total time=   2.2s\n",
      "[CV 4/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.680 total time=   1.6s\n",
      "[CV 5/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.699 total time=   1.4s\n",
      "[CV 1/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=1.000 total time=   1.3s\n",
      "[CV 2/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.750 total time=   1.4s\n",
      "[CV 3/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.524 total time=   1.3s\n",
      "[CV 4/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.680 total time=   1.6s\n",
      "[CV 5/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.699 total time=   2.6s\n",
      "[CV 1/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=1.000 total time=   2.8s\n",
      "[CV 2/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.750 total time=   1.6s\n",
      "[CV 3/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.524 total time=   1.1s\n",
      "[CV 4/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.680 total time=   1.7s\n",
      "[CV 5/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.301 total time=   1.2s\n",
      "[CV 1/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=1.000 total time=   1.3s\n",
      "[CV 2/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.750 total time=   1.6s\n",
      "[CV 3/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.524 total time=   1.6s\n",
      "[CV 4/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.680 total time=   3.1s\n",
      "[CV 5/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.699 total time=   2.8s\n",
      "[CV 1/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=1.000 total time=   1.2s\n",
      "[CV 2/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.750 total time=   1.2s\n",
      "[CV 3/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.524 total time=   1.5s\n",
      "[CV 4/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.680 total time=   1.2s\n",
      "[CV 5/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.699 total time=   1.2s\n",
      "[CV 1/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=1.000 total time=   1.3s\n",
      "[CV 2/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.750 total time=   2.5s\n",
      "[CV 3/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.524 total time=   2.5s\n",
      "[CV 4/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.680 total time=   2.7s\n",
      "[CV 5/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.699 total time=   1.3s\n",
      "[CV 1/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=1.000 total time=   1.2s\n",
      "[CV 2/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.750 total time=   1.4s\n",
      "[CV 3/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.524 total time=   1.3s\n",
      "[CV 4/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.680 total time=   1.2s\n",
      "[CV 5/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.699 total time=   1.3s\n",
      "[CV 1/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=1.000 total time=   2.6s\n",
      "[CV 2/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.750 total time=   2.5s\n",
      "[CV 3/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.524 total time=   2.4s\n",
      "[CV 4/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.680 total time=   1.1s\n",
      "[CV 5/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.699 total time=   1.2s\n",
      "[CV 1/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.952 total time=   1.7s\n",
      "[CV 2/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.808 total time=   1.3s\n",
      "[CV 3/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.806 total time=   1.2s\n",
      "[CV 4/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.796 total time=   1.3s\n",
      "[CV 5/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.864 total time=   2.8s\n",
      "[CV 1/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.971 total time=   2.5s\n",
      "[CV 2/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.837 total time=   1.5s\n",
      "[CV 3/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.854 total time=   1.1s\n",
      "[CV 4/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.835 total time=   1.2s\n",
      "[CV 5/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.854 total time=   1.5s\n",
      "[CV 1/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=1.000 total time=   1.1s\n",
      "[CV 2/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.798 total time=   1.3s\n",
      "[CV 3/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.816 total time=   1.3s\n",
      "[CV 4/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.874 total time=   2.8s\n",
      "[CV 5/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8;, score=0.883 total time=   2.7s\n",
      "[CV 1/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=1.000 total time=   1.8s\n",
      "[CV 2/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.798 total time=   1.2s\n",
      "[CV 3/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.786 total time=   1.1s\n",
      "[CV 4/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.845 total time=   1.5s\n",
      "[CV 5/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2;, score=0.864 total time=   1.2s\n",
      "[CV 1/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.962 total time=   1.3s\n",
      "[CV 2/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.817 total time=   1.1s\n",
      "[CV 3/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.777 total time=   2.0s\n",
      "[CV 4/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.825 total time=   2.5s\n",
      "[CV 5/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4;, score=0.874 total time=   2.4s\n",
      "[CV 1/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.923 total time=   1.3s\n",
      "[CV 2/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.817 total time=   1.1s\n",
      "[CV 3/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.796 total time=   1.4s\n",
      "[CV 4/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.825 total time=   1.3s\n",
      "[CV 5/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8;, score=0.854 total time=   1.4s\n",
      "[CV 1/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.990 total time=   1.2s\n",
      "[CV 2/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.798 total time=   1.5s\n",
      "[CV 3/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.777 total time=   2.6s\n",
      "[CV 4/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.767 total time=   2.5s\n",
      "[CV 5/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2;, score=0.864 total time=   1.6s\n",
      "[CV 1/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.981 total time=   1.2s\n",
      "[CV 2/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.817 total time=   1.6s\n",
      "[CV 3/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.728 total time=   1.2s\n",
      "[CV 4/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.806 total time=   1.3s\n",
      "[CV 5/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4;, score=0.806 total time=   1.3s\n",
      "[CV 1/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.923 total time=   1.4s\n",
      "[CV 2/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.798 total time=   2.1s\n",
      "[CV 3/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.777 total time=   2.4s\n",
      "[CV 4/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.816 total time=   2.1s\n",
      "[CV 5/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8;, score=0.883 total time=   1.2s\n",
      "[CV 1/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.952 total time=   1.5s\n",
      "[CV 2/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.721 total time=   1.1s\n",
      "[CV 3/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.524 total time=   1.4s\n",
      "[CV 4/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.796 total time=   1.2s\n",
      "[CV 5/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2;, score=0.767 total time=   1.4s\n",
      "[CV 1/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.971 total time=   1.4s\n",
      "[CV 2/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.760 total time=   2.5s\n",
      "[CV 3/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.718 total time=   2.5s\n",
      "[CV 4/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.757 total time=   1.4s\n",
      "[CV 5/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4;, score=0.796 total time=   1.5s\n",
      "[CV 1/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.846 total time=   1.1s\n",
      "[CV 2/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.798 total time=   1.1s\n",
      "[CV 3/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.641 total time=   1.2s\n",
      "[CV 4/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.699 total time=   1.4s\n",
      "[CV 5/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8;, score=0.796 total time=   1.3s\n",
      "[CV 1/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.817 total time=   1.7s\n",
      "[CV 2/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.779 total time=   2.5s\n",
      "[CV 3/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.524 total time=   2.4s\n",
      "[CV 4/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.718 total time=   1.4s\n",
      "[CV 5/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2;, score=0.748 total time=   1.3s\n",
      "[CV 1/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.817 total time=   1.2s\n",
      "[CV 2/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.769 total time=   1.2s\n",
      "[CV 3/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.631 total time=   1.6s\n",
      "[CV 4/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.767 total time=   1.2s\n",
      "[CV 5/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4;, score=0.767 total time=   1.2s\n",
      "[CV 1/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.798 total time=   2.5s\n",
      "[CV 2/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.760 total time=   2.5s\n",
      "[CV 3/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.670 total time=   1.9s\n",
      "[CV 4/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.748 total time=   1.1s\n",
      "[CV 5/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8;, score=0.748 total time=   1.2s\n",
      "[CV 1/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.942 total time=   1.3s\n",
      "[CV 2/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.769 total time=   1.6s\n",
      "[CV 3/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.621 total time=   1.2s\n",
      "[CV 4/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.699 total time=   1.2s\n",
      "[CV 5/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2;, score=0.621 total time=   1.8s\n",
      "[CV 1/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=1.000 total time=   2.6s\n",
      "[CV 2/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.760 total time=   2.6s\n",
      "[CV 3/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.670 total time=   1.4s\n",
      "[CV 4/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.718 total time=   1.1s\n",
      "[CV 5/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4;, score=0.670 total time=   1.4s\n",
      "[CV 1/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.808 total time=   1.4s\n",
      "[CV 2/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.740 total time=   1.3s\n",
      "[CV 3/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.650 total time=   1.2s\n",
      "[CV 4/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.718 total time=   1.3s\n",
      "[CV 5/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8;, score=0.680 total time=   2.5s\n",
      "[CV 1/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=1.000 total time=   2.7s\n",
      "[CV 2/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.750 total time=   1.5s\n",
      "[CV 3/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.524 total time=   1.2s\n",
      "[CV 4/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.680 total time=   1.4s\n",
      "[CV 5/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2;, score=0.699 total time=   1.4s\n",
      "[CV 1/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=1.000 total time=   1.2s\n",
      "[CV 2/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.750 total time=   1.2s\n",
      "[CV 3/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.524 total time=   1.2s\n",
      "[CV 4/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.680 total time=   1.7s\n",
      "[CV 5/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4;, score=0.699 total time=   3.2s\n",
      "[CV 1/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=1.000 total time=   2.3s\n",
      "[CV 2/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.750 total time=   1.4s\n",
      "[CV 3/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.524 total time=   1.3s\n",
      "[CV 4/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.680 total time=   1.7s\n",
      "[CV 5/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8;, score=0.699 total time=   1.4s\n",
      "[CV 1/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.000 total time=   1.5s\n",
      "[CV 2/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.250 total time=   1.6s\n",
      "[CV 3/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.524 total time=   2.9s\n",
      "[CV 4/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.680 total time=   2.9s\n",
      "[CV 5/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2;, score=0.699 total time=   1.3s\n",
      "[CV 1/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=1.000 total time=   1.2s\n",
      "[CV 2/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.750 total time=   1.2s\n",
      "[CV 3/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.524 total time=   1.5s\n",
      "[CV 4/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.680 total time=   1.3s\n",
      "[CV 5/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4;, score=0.699 total time=   1.3s\n",
      "[CV 1/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=1.000 total time=   1.7s\n",
      "[CV 2/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.750 total time=   3.0s\n",
      "[CV 3/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.524 total time=   3.0s\n",
      "[CV 4/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.680 total time=   1.3s\n",
      "[CV 5/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8;, score=0.699 total time=   1.3s\n",
      "[CV 1/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=1.000 total time=   1.2s\n",
      "[CV 2/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.750 total time=   1.8s\n",
      "[CV 3/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.524 total time=   1.9s\n",
      "[CV 4/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.680 total time=   3.0s\n",
      "[CV 5/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2;, score=0.699 total time=   2.6s\n",
      "[CV 1/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=1.000 total time=   2.8s\n",
      "[CV 2/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.750 total time=   1.6s\n",
      "[CV 3/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.524 total time=   1.2s\n",
      "[CV 4/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.680 total time=   1.2s\n",
      "[CV 5/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4;, score=0.699 total time=   1.3s\n",
      "[CV 1/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8;, score=1.000 total time=   1.4s\n",
      "[CV 2/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.750 total time=   1.2s\n",
      "[CV 3/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.524 total time=   2.3s\n",
      "[CV 4/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.680 total time=   2.5s\n",
      "[CV 5/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8;, score=0.699 total time=   2.0s\n",
      "[CV 1/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.971 total time=   1.7s\n",
      "[CV 2/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.837 total time=   1.2s\n",
      "[CV 3/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.757 total time=   1.2s\n",
      "[CV 4/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.874 total time=   1.2s\n",
      "[CV 5/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2;, score=0.835 total time=   1.4s\n",
      "[CV 1/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 38/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4;, score=0.981 total time=   1.2s\n",
      "[CV 2/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.0, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)\n",
    "\n",
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a64d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
